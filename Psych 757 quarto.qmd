---
title: "Mapping Polarization Around the 2024 U.S. Presidential Debates"
subtitle: "LLM-Coded Evidence on Debate-Driven Polarization and Sentiment"
author:
  - name: "Diego A. Mazorra"
    email: "diego.mazorra@wisc.edu"
    affiliation: "University of Wisconsin Madison"
date: last-modified
abstract: >
 Presidential debates no longer just inform; they trigger a real-time cascade that sorts audiences, hardens camps, and drenches the conversation in negativity.This study maps polarization and sentiment in real-time social media discourse around the 2024 U.S. presidential debates (Biden–Trump; Harris–Trump). We analyze ~2.24 million tweets collected from one hour before to one hour after each debate, coding tweet-level ideology and a 15-point polarization scale with large language models (LLMs).The results show that debate talk is dominated by medium-intensity polarization, reliably tilting left in volume, and that the more polarized the discourse, the more negative its tone, when sentiment is mildly negative on average (mean compound ≈ −0.11) and becomes progressively more negative as polarization increases. Debate talk skews left overall, with modest shifts across the two events. Strikingly, the two debates produced almost the same structure of online reactions—suggesting that the format itself, more than the candidates, governs how social media processes these events. Methodologically, large language models make this scale of analysis possible but competing models disagree in consequential ways unless calibrated and adjudicated. Substantively, the findings recast debates with predictable spikes of polarized, negative content.This research contributes a detailed snapshot of the political discourse and presents a robust, replicable framework for analyzing social media data. The resulting dataset and methodology offer a valuable resource for future studies in political science and computational communication.
keywords:
  - polarization
  - sentiment analysys
  - scale development
  - political communication
  - R
  - quarto
execute:
  engine: knitr
  echo: true
  warning: false
format:
  html:
    toc: true
    code-fold: true
    toc-depth: 3
    toc-title: "Table of Contents"
    number-sections: true
    code-tools: true
    keep-md: true
    embeded-resources: true
  pdf:
    toc: true
    number-sections: true
    documentclass: "article"
  docx:
    reference-doc: "P757-FinalWork-Diego_Mazorra.docx"
#engine: jupyter # Note: while this were in the example of class, it provoque several conflicts (have to ask to GPT for help here)
bibliography: references.bib
csl: apa.csl
#jupyter: python3 # Note: while this were in the example of class, it provoque several conflicts (have to ask to GPT for help here)
---

# Introduction

The 2024 U.S. presidential debate between Joe Biden, Kamala Harris, and Donald Trump ignited an immediate and intense reaction on social media. The viral nature of these reactions, fueled by memes, video clips, and rapid user-generated commentary, and on these we can found evidence of polarization. While historical research suggests that presidential debates typically have "minimal effects" on voter behavior, Biden's withdrawal from the race weeks later signals an unprecedented shift in the influence of social media on electoral outcomes. This hybrid media environment raises a core question: how does real-time digital discourse register and amplify polarization around high-salience political events?

This study examines how real-time digital discourse reacts to a televise debate with one of the most polarized political decision-making in the wake of one of the most consequential debates in American electoral history. Specifically, it analyzes large-scale Twitter/X discourse surrounding the 2024 debates to map the ideological balance of the conversation and the intensity of polarization, and to test whether polarization is associated with more negative sentiment. Beyond describing these patterns, we assess whether the debates differed in their online profiles, for example if Biden–Trump and Harris–Trump attract different mixtures of left/right voices or levels of polarized talk? By integrating framing analysis and computational approaches, this project provides a methodological advancement in political communication research, offering novel insights into the hybrid media environment that increasingly dictates electoral politics.

We situate the work in comparative political communication. Prior research on civic culture and media systems underscores that information flows and communication styles travel across contexts [@almond1989; @rojas2019]. The project forms part of a broader, ongoing program on debates, populist rhetoric, and polarization and advance the creation of new scales to measure polarization and how include machine learning process in the analysis of big data, including extensions to Latin American cases and multimodal debate analysis [@rojas2025; @mazorra2025]. The present article focuses on the U.S. debates as a high-information test case while developing methods that generalize to cross-national comparisons.

For this reason we ask the next questions:

**RQ1.** How do ideological orientation and polarization intensity manifest in large-scale Twitter discourse surrounding the 2024 U.S. presidential debates, and do these patterns differ between the Biden–Trump and Harris–Trump debates?

**RQ2.** How is sentiment distributed across ideological orientations and polarization levels, and to what extent is higher polarization associated with more negative sentiment?

To explore the potential of Artificial Intelligence (AI) and large language models (LLMs) as tools for analyzing social media data, this study serves as an exploratory pilot, testing their application as coding assistants for multimodal analysis. LLMs like GPT-4 from OpenAI or Gemma3 from Gemini models are probabilistic systems that generate outputs based on contextual patterns and word frequencies. This nature highlights the importance of precise input prompts to guide the model’s interpretation. For this work, GPT-4 was employed as a coder to analyze multimodal features of tweets collected from Syntehsio covering the hour during the presidential debates in the United States in 2024 (one hour before and one hour after the debate).

An example image from the dataset was selected to test the model’s capabilities. The approach involved creating a prompt that included a description of polarization to classify as context for the model’s task. GPT-4 was then instructed to act as a social science coder specializing in communication to classify the tweet on 2 questions on polarization. The prompt was also introduced to a local model Gemma3:12b running on Ollama architecture to compare both outputs and the reliability of the codification. 

According to this objective, we ask the next questions:

**RQ3.** To what degree do large language models (LLMs) agree when coding political polarization in social media content, and what are the implications of inter-model disagreement for the reliability of automated annotation?

**RQ4.** How does GPT-4 perform relative to an open-weight LLM (Gemma3:12B) in coding political polarization, and what potential improvements, such as LoRA fine-tuningm could enhance the performance of smaller models for this task?

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(ggplot2)
library(psych)
library(pheatmap)
library(modelsummary)
library(flextable)
library(apaTables)
library(sjPlot)
library(broom)
library(dplyr)
library(knitr)
library(gt)
library(tibble)

```

# Literature Review

## Televised Presidential Debates

Presidential debates, long considered pillars of democratic discourse, are increasingly present in a hybrid media ecology where both legacy (as radio and televised debates) and digital platforms shape, and sometimes distort, their civic function. Classical theories of democratic deliberation position debates as tools of voter enlightenment, promoting informed decisions and accountability [@olson2025]. Yet, the assumption that debates function uniformly as rational public spheres and have strong effects has been increasingly challenged[@cantu2023]. We argue that debates may no longer reflect democratic discussion but perform a precarious communicative balancing act. Their function is now modulated by real-time interactions across platforms, by the volatility of attention, and by emotional resonance rather than policy clarity [@jimenez-preciado2025; @lukito2022]. Even when a debate is not necessarily polarized, the public can shape its interpretation according to the polarization context of society.

While debates have historically operated as sites of image restoration and rhetorical confrontation [@martinezabellan2024], their dynamics have grown more affective and fragmented, with online discourse extending and refracting their impact far beyond the televised moment [@eaton2024]. At the same time, debates serve as discursive battlegrounds that reflect affective polarization, not just partisan disagreement [@brooks2024]. The shift to platforms like YouTube and Twitter has revealed how mediated intergroup contact can either mitigate or exacerbate affective tensions depending on narrative framing and sequence of exposure. This environment challenges journalistic norms as well: although debates offer a rare moment of unscripted candidate performance, the role of journalists and moderators is contested, sometimes triggering backlash rather than reinforcing accountability [@mckinney2024; @franco-hantzsch2022]. As some researchers argue, debates still hold the potential to shape voter decisions, particularly in weakly institutionalized systems [@cantu2023], but this potential is increasingly contingent on their broader amplification across media layers, rather than confined to the debate stage alone. Theoretical frameworks that engage with debates today must therefore contend with the interplay between traditional broadcast norms, emergent affective dynamics, and the algorithmic structuring of public discourse.

The electoral debate can be seen as a media event that generates political expression on various screens, from TV to social media [@wells2016]. There is a relationship between the electoral context, the gestures of the candidate rather than speech content, and the response volume and valence to social media [@shah2016], and how this populist communication style drives attention on Twitter during political debates and how people response differently to the visual gestures, tone and verbal dimension of the debate in social media [@bucy2020]. To analyze this problem, @shah2023 demonstrate the effectiveness of multimodal classifiers in analyzing political debates, considering visual, audio, and textual features, and @lukito2022 shows the power of social media data in the analyses of such debates .

By integrating these perspectives, this study integrates theories of agenda-setting and framing [@mccombs1972], dual screening and media co-production [@wells2016], and affective intelligence [@marcus2000] to explain how digital platforms mediate real-time political reactions. The study also draws from visual communication theory [@bucy2020] to assess how emotional appeals, candidate performance, and framing influence public engagement. The electoral debate can be seen as a media event that generates political expression on various screens, from TV to social media [@wells2016; @lukito2022; @jimenez-preciado2025]. There is a relationship between the electoral context and the response volume and valence to social media [@shah2016], and how this populist communication style drives attention on Twitter during political debates and how people response differently to the debate in social media [@bucy2020]. To analyze this problem, @shah2023 demonstrate the effectiveness of multimodal classifiers in analyzing political debates, considering visual, audio, and textual features. By integrating these perspectives, the project offers a novel approach to studying the impact of populist rhetoric in televised presidential debates.

## Polarization

The political debates in America in 2024 occur not in a vacuum but within a polarized society, as evidenced by the total number of votes. Additionally, these debates take place in a hybrid media environment dominated by digital communication. With the rise of digital media and a networked public sphere, scholars warned that changing information flows could reshape how citizens sort and see one another [@FriedlandHoveRojas2006; @Sunstein2007]. Early evidence of ideological clustering online and partisan sorting in the United States made these concerns concrete [@abramowitz1998; @AdamicGlance2005; @FiorinaAbrams2008]. To evaluate such shifts and their democratic implications, it is essential to measure polarization, and to be clear about which form we are measuring and in the context we are measuring it [@rojas2025].

One way to see polarization is to distinguish attitudinal (issue-based) polarization (greater extremity or distance in policy preferences) from other forms of polarization like ideological or affective [@AbramowitzSaunders2008; @abramowitz2016]. Debate persists over its extent in the U.S. electorate [@FiorinaAbrams2008], and cross-national comparisons show important variation and citizens’ sensitivity to it [@Lupu2015]. Researchers therefore also measure perceived polarization, what people think the level of polarization is [@WestfallEtAl2015; @YangEtAl2016; @ArmalyEnders2021], because perceptions can move independently of actual issue extremity and still shape behavior.

A third strand focuses on affective polarization, this is how much partisans dislike one another. Defined as the tendency to evaluate opposing identifiers as a disliked out-group, affective polarization has risen markedly in the U.S. [@IyengarSoodLelkes2012]. Evidence suggests linkages among forms, like ideological polarization predicting affective polarization, and affective polarization predicting perceived polarization, underscoring the need to measure each separately and jointly [@AlgaraZur2023; @ArmalyEnders2021]. Long-run, cross-country evidence also shows that trajectories differ across contexts and track elite polarization, reinforcing the value of comparable measurement over time and place [@BoxellGentzkowShapiro2022].

Social media is often portrayed as an engine of polarization: selective exposure, partisan media, and homophilous discussion networks could create echo chambers [@Sunstein2007; @Levendusky2013; @LeeCho2023]). Yet the causal order between selective exposure and polarization remains contested [@Stroud2010], and real-world findings are mixed. In Latin America, several studies report little systematic link between routine media or platform use and affective polarization [@SchermanEtAl2022; @ValenzuelaBachmannBargsted2021; @rojas2025]. At the same time, today’s media environment can heighten perceived polarization by spotlighting extreme partisan cases [@YangEtAl2016] and by hostile interpretations among partisans [@TongWincklerRojas2021]. Misinformation concerns further complicate inference, with evidence that social media use does not uniformly increase misinformed beliefs [@ValenzuelaMunizSantos2022] and that the affectively polarized may be more prone to produce misinformation [@OsmundsenEtAl2021]. These patterns strengthen the case for measuring multiple facets (attitudinal, affective, and perceived polarization) within social media contexts rather than assuming one pathway.

Polarization affects core democratic goods. When groups doubt one another’s competence or good faith, deliberation, requiring publicity, equality, rationality, and argument grounded in the common good, erodes [@MendelbergOleske2000]. Higher polarization is associated with lower democratic support and more openness to illiberal remedies, making early detection and tracking crucial [@TorcalMagalhaes2022; @Wagner2021]. At the same time, polarization can mobilize citizens, increasing turnout and other forms of participation, which also warrants measurement to assess trade-offs [@HarteveldWagner2023].

Beyond citizens’ polarization, perceived media polarization (how people judge the media system’s ideological tilt) has distinct antecedents and consequences. Perceptions can form through meta-coverage that emphasizes bias and conflict [@WattsDomkeShahFan1999], through hostile media judgments [@ValloneRossLepper1985], or against a background of debates over objective bias [@Dennis1997; @DAlessioAllen2000; @Entman1996; @FicoFreedman2008]. Because perceptions of bias shape how audiences use and evaluate outlets [@HoEtAl2011; @StroudMuddimanLee2014], measuring perceived media polarization is indispensable for linking the media system to affective and perceived societal polarization and to participation [@TongWincklerRojas2021].

In asymmetric contexts, where elites and observers see movement toward one pole, perceptions can also feed back into representation and participation. Work on asymmetric polarization argues that conservative-leaning climates of opinion can arise when elites misread public preferences based on limited or biased cues [@HackerPierson2015; @BroockmanSkovron2018]. Tracking perceived media polarization alongside affective and attitudinal measures is therefore critical, especially in comparative settings such as Latin America where trajectories and media–politics linkages differ [@BoxellGentzkowShapiro2022; @rojas2019; @rojas2025]

# Methods

This study utilizes a multi-stage computational approach to analyze the content of 2,238,712 tweets related to the 2024 U.S. presidential debates compiled by usign Synthesio on the SMAD (Social Media and Democracy) research group at the university of Wisconsin-Madison. The methodology encompasses data collection and preprocessing, AI-powered annotation, development of a polarization classification model, and sentiment analysis that can be found in the jupyter lab project in git hub that goes along with this article.

The initial dataset was compiled by merging multiple CSV files of tweets collected via the Synthesio platform during the two 2024 presidential debates (Biden-Trump and Harris-Trump). These files were merged to create a unified dataset. Raw tweet data was processed to generate a “santext” column for analysis. This process involved the following steps: de-duplication based on unique tweet IDs, removal of non-analyzable tweets (e.g., those marked "Deleted or protected mention"), and conversion to lowercase, followed by the removal of URLs and special characters using regular expressions. A random sample of 1000 tweets was subsequently extracted from this cleaned dataset for the annotation phase.

To create a labeled dataset for model training, Large Language Models (LLMs) were employed as expert coders. A subset of the tweets was annotated using two different models to assess their capabilities and evaluate inter-coder reliability. The seelcted 1000 tweet sample was processed via the OpenAI API, utilizing the GPT-4 model. The model was instructed to classify each tweet's according to left and right ideology content and according to a polarization score using a detailed codebook defining scores ranging from -7 (strongly left) to +7 (strongly right), with a neutral classification, and to provide its response in a structured JSON format. The GPT-4 annotated data served as the ground truth for training the final classification models.

For comparison, a local Gemma 3:12B model was used via Ollama to classify a 100-tweet subset of the same sample using a few-shot prompting technique. Inter-coder reliability between the GPT-4 and Gemma annotations was evaluated using Krippendorff’s alpha for content analyses. A resulting alpha of -0.466 for the ordinal polarization scores indicated significant disagreement between the two models, highlighting the methodological challenges of using different LLMs for classification tasks.

Several machine learning models were evaluated to identify the most effective classifier for polarization, Naive Bayes, Bert and LightGBM (Light Gradient-Boosting Machine) are the models reported in the study [@meng2016; @shi2022; @ke2017; @zhang2018]. While baseline models, including Logistic Regression and Naive Bayes, were tested. A Naive Bayes model was selected for a classification fo the tweets between left and right, and the final model combined BERT embeddings with a LightGBM classifier for the polarization score . The text of each tweet was transformed via distilBERT into a 768-dimensional numerical vector using the distilBERT-base-uncased model, converting the semantic content of the tweet into a format suitable for machine learning. After that, a LightGBM classifier was trained on these BERT embeddings, The model was trained to categorize tweets into four levels of polarization: None, Low, Medium, and High to accelerate the production of the model. These levels were derived from the original *polar_score* provided by the GPT-4 annotation. The trained model was then applied to the full dataset of over 2.2 million tweets to generate the final polarization labels.

To provide an emotional context to the analysis, sentiment scores were calculated for each tweet using the VADER (Valence Aware Dictionary and Sentiment Reasoner) library, a lexicon and rule-based sentiment analysis tool optimized for social media text that is a method reliable for this type of datasets [@nelson2023]. For each tweet, four scores were generated and appended to the final dataset: negative, neutral, positive, and an overall normalized *vader_compound* score ranging from -1 (most negative) to +1 (most positive).

```{r}
#| label: load-data
#| echo: false


theme_set(theme_minimal(base_size = 14))

# Load df
df <- read_csv("/Users/diego/anaconda_projects/PSYCH 757 FINAL/data/processed/debate_tweets_with_polarization_and_sentiment.csv")

df <- df %>%
  mutate(
    debate_label = factor(Debate,
                          levels = c(1, 2),
                          labels = c("Biden vs. Trump", "Harris vs. Trump")),
    
    ideology = factor(predicted_binary_label,
                      levels = c("left", "right")),

    polarization_level = factor(predicted_polarization_level,
                                levels = c("Low", "Medium", "High", "None"),
                                ordered = TRUE)
  )

polarized_df <- df %>%
  filter(polarization_level %in% c("Low", "Medium", "High"))
```

The following table provides an excerpt of the final processed dataset. It displays five sample tweets after sanitization, along with their final predicted classifications for ideological leaning, polarization intensity, and sentiment. This offers a concrete illustration of the data structure that forms the basis for the subsequent analyses.

```{r}
#| label: tbl-data-excerpt
#| tbl-cap: "Excerpt of the Final Processed Tweet Dataset"
#| echo: false


df %>%
  select(
    "Tweet Text" = santext,
    "Debate" = debate_label,
    "Ideology" = ideology,
    "Polarization" = polarization_level,
    "Sentiment Score" = vader_compound
  ) %>%
  head(5) %>% #selecxt 5 rows to show in the table
  gt() %>%
  fmt_number(
    columns = `Sentiment Score`,
    decimals = 3
  ) %>%
  tab_options(
    table.width = pct(100) # Make table fill the width
  )
```

To ensure clarity and reproducibility, the following table serves as a data dictionary. It formally defines each of the key variables used throughout this study, including their description and data type. This provides essential context for interpreting the statistical results.

```{r}
#| label: tbl-data-dictionary
#| tbl-cap: "Data Dictionary for Key Analytical Variables"
#| echo: false

# dictionary
data_dictionary <- tibble(
  Variable = c(
    "debate_label", 
    "santext",
    "polar_score",
    "ideology", 
    "polarization_level", 
    "vader_compound",
    "vader_neg",
    "vader_neu",
    "vader_pos",
    "Potential Reach"
  ),
  Description = c(
    "Categorical variable identifying which of the two presidential debates the tweet is associated with.",
    "The sanitized tweet text, used as the input for all NLP and sentiment models.",
    "The original numerical score (-7 to +7) assigned by the GPT-4 annotator, used to derive the polarization_level.",
    "The predicted ideological leaning of the tweet, classified as 'left' or 'right' by the final model.",
    "The predicted intensity of polarization, classified into four ordered levels: 'None', 'Low', 'Medium', or 'High'.",
    "The normalized compound sentiment score from VADER, ranging from -1 (most negative) to +1 (most positive).",
    "The proportion of the tweet's text classified as negative by VADER.",
    "The proportion of the tweet's text classified as neutral by VADER.",
    "The proportion of the tweet's text classified as positive by VADER.",
    "An estimate of the potential audience size for the tweet, based on the publisher's follower count."
  ),
  Type = c(
    "Categorical (Factor)", 
    "Character (String)",
    "Numeric (Integer)",
    "Categorical (Factor)", 
    "Ordinal (Ordered Factor)", 
    "Continuous (Numeric)",
    "Continuous (Numeric)",
    "Continuous (Numeric)",
    "Continuous (Numeric)",
    "Numeric (Integer)"
  )
)

# Create the gt table
gt(data_dictionary) %>%
  cols_label(
    Variable = "Variable Name",
    Description = "Description",
    Type = "Data Type"
  ) %>%
  tab_options(
    table.width = pct(100) # Ensure table uses full width for better readability
  )
```

# Results

To asess the agreement between GPT-4 and the locally run Gemma3:12B model in polarization classification, a sample of 100 tweets was annotated by both models and compared using Krippendorff’s alpha for ordinal data [@krippendorff1977; @krippendorff2004]. The resulting coefficient was **-**0.466, indicating substantial disagreement well below the conventional threshold for acceptable reliability (typically α ≥ 0.67).

Figure 1 visualizes this disagreement by plotting GPT-4’s assigned polarization scores on the x-axis and Gemma3:12B’s scores on the y-axis, with the red dashed line representing perfect agreement (1:1 correspondence). The scatter plot shows a wide dispersion of points away from the diagonal, with clusters at extreme values suggesting that while both models sometimes identified strong polarization, they often differed in direction (left vs. right) and intensity. This pattern underscores the methodological challenge of using different large language models for consistent polarization measurement.

![Figure 1: LLM Comparison](Figure 1.png){fig-alt="LLM Comparison"}

Divergences occurred in both the direction (left vs. right) and the magnitude (low vs. high) of polarization. This highlights the methodological risk of assuming that different LLMs will converge in classification without extensive calibration. GPT-4 provided more stable and internally consistent annotations aligned with the intended coding framework for ideological orientation and polarization. In contrast, Gemma3:12B, as a smaller and less specialized model, showed greater variability and misalignment. While these results underscore GPT-4’s stronger performance in this domain, they also point to the potential of improving smaller, open-weight models like Gemma3 via targeted adaptation techniques such as LoRA fine-tuning to close the performance gap.

Across models, tree-based and embedding-based approaches outperformed agians the other models. On the binary left–right task, Logistic Regression reached 0.71 accuracy (macro-F1 = 0.71) while Multinomial Naive Bayes did better at 0.75 (macro-F1 = 0.75) on the 175-tweet test set, so Naive Bayes was chosen as the binary champion model. When predicting the full ordinal polarization score with Naive Bayes, performance collapsed (accuracy = 0.41; macro-F1 = 0.12 on 182 tweets), signaling that bag-of-words features struggle with fine-grained intensity. Collapsing to four levels (None/Low/Medium/High) and switching to LightGBM markedly improved results (accuracy = 0.61; macro-F1 = 0.58), with strongest recall on Medium (0.70) and best precision on High (0.78) but lower recall there (0.47). Using BERT embeddings, the illustrative BERT + Logistic Regression run reached \~0.60 accuracy on the same 4-level task, and the pipeline promotes the BERT + LightGBM variant as the production champion model for labeling the full 2.24M-tweet corpus. These patterns motivate the final choice of a BERT-embedded LightGBM classifier: it preserves most of the binary gains from NB while scaling better to ordinal intensity than bag-of-words baselines.   

Aftter this results, human coder revise the divergences and choose the code of GPT-4. We begin by exploring the overall characteristics of the dataset to understand the general nature of the conversation. First, we examine the overall composition of the Twitter discussion. Analysis of 2.2 million tweets from the 2024 U.S. presidential debates, 60.42% were coded as left-leaning and 39.58% as right-leaning (Table 1). Ideological balance varied by event: during the Biden–Trump debate, left-leaning tweets comprised 64.7% (445,329/688,251) versus 58.5% (907,358/1,550,461) during Harris–Trump. A χ² test confirmed dependence between debate and ideology (χ²=7,618.93, df=1, p\<.001). The table 1 below show the frequency and percentage of tweets classified with a left or right ideological leaning.

```{r}
#| label: tbl-ideology
#| tbl-cap: "Frequency of Ideological Labels"
#| echo: false

df %>%
  filter(!is.na(ideology)) %>%
  count(ideology) %>%
  mutate(percentage = n / sum(n) * 100) %>%
  kable(digits = 2)
```

Second, we show the distribution of polarization intensity among the tweets that showed clear polarization. The proportional distribution of polarization intensity was remarkably consistent between the Biden–Trump and Harris–Trump debates, suggesting that while ideological representation differed significantly and more *left*-leaning tweets in the Biden–Trump debate, the level of polarization was largely stable across events, as we can see in the next table.

```{r}
#| label: tbl-polarization
#| tbl-cap: "Distribution of Polarization Levels"
#| echo: false

polarized_df %>%
  count(polarization_level) %>%
  mutate(percentage = n / sum(n) * 100) %>%
  kable(digits = 2)
```

Collapsing the 15-point polarization scale to three intensity levels, Medium polarization dominated the conversation (51.87%), followed by Low (45.55%) and High (2.58%) (Table 2). Intensity distributions were highly similar across the two debates—Biden–Trump: Low 43.6%, Medium 54.0%, High 2.36%; Harris–Trump: Low 46.4%, Medium 50.9%, High 2.67%. Although the distribution differed statistically by event (χ²=1,859.45, df=2, p\<.001), the practical magnitude was very small (Cramér’s V≈0.029), suggesting broad stability in polarization intensity across debate pairings.

To understand if the conversational dynamics shifted between the two events, we can visualize the proportion of polarization levels for each debate. The chart below illustrates whether one debate prompted a more intensely polarized reaction than the other.

```{r}
#| label: plot-polarization
#| fig-cap: "Proportion of Polarization Levels by Debate"
#| echo: false

# Stacked bar chart
ggplot(polarized_df, aes(x = debate_label, fill = polarization_level)) +
  geom_bar(position = "fill") +
  labs(title = "Proportion of Polarization Levels by Debate",
       x = "Debate",
       y = "Proportion of Tweets",
       fill = "Polarization Level") +
  scale_y_continuous(labels = scales::percent)
```

The general tone of the conversation is assessed using the VADER compound sentiment score, which ranges from -1 (most negative) to +1 (most positive). Overall tone was slightly negative (VADER compound: M=−0.111; Median=0.000; SD=0.499), with a heavy mass at 0.0 reflecting neutral or mixed content (Table 3). Left-leaning tweets were more negative on average than right-leaning tweets (−0.122 vs. −0.094), a difference that was statistically reliable (t=−42.03, df≈2,010,040, p\<.001; Table 6). High polarization tweets were markedly more negative than both low and medium polarization tweets, supporting the idea that affective polarization is tied to more hostile or antagonistic discourse. The table 3 below provides the key summary statistics for this score.

```{r}
#| label: tbl-vader-summary
#| tbl-cap: "Summary Statistics for VADER Compound Sentiment Score"
#| echo: false

df %>%
  summarise(
    Mean = mean(vader_compound, na.rm = TRUE),
    Median = median(vader_compound, na.rm = TRUE),
    "Std. Deviation" = sd(vader_compound, na.rm = TRUE)
  ) %>%
  gt() %>%
  fmt_number(
    columns = everything(),
    decimals = 3 
  )
```

To visualize the spread of these scores, the histogram below shows the distribution of tweets across the sentiment spectrum. The plot highlights a large concentration of tweets at the neutral point (0.0) and reveals the overall shape and skew of the sentiment in the discussion.

```{r}
#| label: fig-vader-histogram
#| fig-cap: "Distribution of VADER Compound Sentiment Scores"
#| echo: false

ggplot(df, aes(x = vader_compound)) +
  geom_histogram(bins = 50, fill = "skyblue", color = "black") +
  labs(title = "Distribution of VADER Compound Sentiment Scores",
       x = "Compound Sentiment Score (-1 to +1)",
       y = "Number of Tweets")
```

Sentiment varied monotonically with polarization intensity. A one-way ANOVA showed a strong overall effect (F(2,·)=17,777.69, p\<.001). Tukey HSD indicated that each pairwise contrast was significant: Medium vs. Low (Δ=−0.110), High vs. Low (Δ=−0.258), and High vs. Medium (Δ=−0.148), confirming that more polarized content is progressively more negative (Table 7 and table 8).

We use the Chi-Square Test of Independence to check if the debate event is associated with the ideological leaning or the polarization level of the tweets. A significant p-value (typically \< 0.05) suggests that the distributions are not the same across the two debates.

```{r}
#| label: tbl-chi-ideology
#| tbl-cap: "Chi-Square Test for Association between Debate and Ideology"
#| echo: false


ideology_debate_table <- table(df$debate_label, df$ideology)

chi_test_ideology <- chisq.test(ideology_debate_table)

tidy_results_ideology <- tidy(chi_test_ideology) %>%
  select(statistic, p.value, parameter) %>%
  rename(
    "Chi-Square (χ²)" = statistic,
    "p-value" = p.value,
    "Degrees of Freedom (df)" = parameter
  ) %>%
  mutate(
    `p-value` = if_else(`p-value` < 0.001, "< .001", as.character(round(`p-value`, 3)))
  )

# Display
knitr::kable(ideology_debate_table, caption = "Contingency Table: Debate vs. Ideology")
knitr::kable(tidy_results_ideology, caption = "Chi-Square Test Results", digits = 2)

```

Next, we test the association between the debate and the intensity of polarization (Low, Medium, High) for tweets that were classified as polarized.

```{r}
#| label: tbl-chi-polarization
#| tbl-cap: "Chi-Square Test for Association between Debate and Polarization Level"
#| echo: false

polarized_df_for_test <- df %>%
  filter(polarization_level %in% c("Low", "Medium", "High")) %>%
  mutate(polarization_level = droplevels(polarization_level))

polarization_debate_table <- table(polarized_df_for_test$debate_label, 
                                     polarized_df_for_test$polarization_level)

chi_test_polarization <- chisq.test(polarization_debate_table)

tidy_results_polarization <- tidy(chi_test_polarization) %>%
  select(statistic, p.value, parameter) %>%
  rename(
    "Chi-Square (χ²)" = statistic,
    "p-value" = p.value,
    "Degrees of Freedom (df)" = parameter
  ) %>%
  mutate(
    `p-value` = if_else(`p-value` < 0.001, "< .001", as.character(round(`p-value`, 3)))
  )

#display 
knitr::kable(polarization_debate_table, caption = "Contingency Table: Debate vs. Polarization Level")
knitr::kable(tidy_results_polarization, caption = "Chi-Square Test Results", digits = 2)
```

Here, we investigate the relationship between the sentiment of a tweet and its ideological or polarization classification.

To determine if a significant difference in sentiment exists between left-leaning and right-leaning tweets, we conduct an independent samples t-test. The statistical results, including the means for each group, the t-statistic, and the p-value, are presented in the table below.

```{r}
#| label: tbl-t-test-ideology
#| tbl-cap: "T-test Results for Sentiment Scores by Ideological Label"
#| echo: false

t_test_ideology <- t.test(vader_compound ~ ideology, data = df)

tidy(t_test_ideology) %>%
  select(estimate1, estimate2, statistic, p.value, parameter) %>%
  rename(
    "Mean Sentiment (Left)" = estimate1,
    "Mean Sentiment (Right)" = estimate2,
    "t-statistic" = statistic,
    "p-value" = p.value,
    "df" = parameter
  ) %>%
  mutate(
    `p-value` = if_else(`p-value` < 0.001, "< .001", as.character(round(`p-value`, 3)))
  ) %>%
  kable(digits = 3) 
```

The box plot below provides a visual comparison of the sentiment score distributions for left- and right-leaning tweets. It clearly shows the median, interquartile range, and outliers for each group, complementing the statistical results from the t-test.

```{r}
#| label: fig-boxplot-ideology
#| fig-cap: "Distribution of VADER Sentiment Scores by Ideological Label"
#| echo: false

# Box plot 
ggplot(df, aes(x = ideology, y = vader_compound, fill = ideology)) +
  geom_boxplot() +
  labs(title = "Sentiment Score by Ideological Label",
       x = "Ideological Label",
       y = "VADER Compound Score") +
  theme(legend.position = "none")
```

To test whether sentiment differs significantly across the Low, Medium, and High polarization levels, a One-Way Analysis of Variance (ANOVA) was conducted. The results, summarized in the table below, show a statistically significant effect of polarization level on the VADER compound sentiment score.

```{r}
#| label: tbl-anova-polarization
#| tbl-cap: "ANOVA Results for Sentiment Scores by Polarization Level"
#| echo: false

anova_polarization <- aov(vader_compound ~ polarization_level, data = polarized_df)

tidy(anova_polarization) %>%
  select(term, df, statistic, p.value) %>%
  rename(
    "Source of Variation" = term,
    "df" = df,
    "F-statistic" = statistic,
    "p-value" = p.value
  ) %>%
  mutate(
    `p-value` = if_else(`p-value` < 0.001, "< .001", as.character(round(`p-value`, 3)))
  ) %>%
  filter(`Source of Variation` != "Residuals") %>%
  kable(digits = 2)
```

Given the significant result from the ANOVA, a Tukey's HSD post-hoc test was performed to examine the pairwise differences between the polarization levels. The table below shows that the mean sentiment score for each polarization level is significantly different from the others.

```{r}
#| label: tbl-tukey-polarization
#| tbl-cap: "Tukey HSD Post-Hoc Comparisons for Polarization Levels"
#| echo: false

TukeyHSD(anova_polarization) %>%
  tidy() %>%
  select(contrast, estimate, conf.low, conf.high, adj.p.value) %>% 
  rename(
    "Comparison" = contrast, # Renamed from the 'contrast' column
    "Mean Difference" = estimate,
    "Lower 95% CI" = conf.low,
    "Upper 95% CI" = conf.high,
    "Adjusted p-value" = adj.p.value
  ) %>%
  mutate(
    `Adjusted p-value` = if_else(`Adjusted p-value` < 0.001, "< .001", as.character(round(`Adjusted p-value`, 3)))
  ) %>%
  kable(digits = 3)
```

The box plot below visually summarizes the sentiment distributions for each polarization level, illustrating the differences identified by the ANOVA and Tukey tests. We can observe a clear trend where higher levels of polarization are associated with more negative sentiment scores. This shows a monotonic relationship between polarization intensity and sentiment: each step up in polarization is associated with a further drop into negative tone. This supports the interpretation that more polarized content is, on average, more negative, and that the effect is broad-based across the distribution.

```{r}
#| label: fig-boxplot-polarization
#| fig-cap: "Distribution of VADER Sentiment Scores by Polarization Level"
#| echo: false

# Box plot 
ggplot(polarized_df, aes(x = polarization_level, y = vader_compound, fill = polarization_level)) +
  geom_boxplot() +
  labs(title = "Sentiment Score by Polarization Level",
       x = "Polarization Level",
       y = "VADER Compound Score") +
  theme(legend.position = "none")
```

# Discussion

This study provides a comprehensive, computationally driven examination of polarization and sentiment in Twitter discourse surrounding the 2024 U.S. presidential debates. By combining large-scale machine learning classification with sentiment analysis, the findings offer empirical insights into how debates are received in the hybrid media environment described by @wells2016 and @lukito2022. In such environments, algorithmic curation and social signaling can heighten the perception of polarization even when extreme content remains a minority of overall discourse [@Sunstein2007; @Stroud2010; @YangEtAl2016].

The predominance of left-leaning tweets (60.4%) and the dominance of medium polarization (51.9%) underscore the extent to which debate discourse is shaped by already engaged and affectively aligned publics [@brooks2024]. The relatively stable polarization distribution across the Biden–Trump and Harris–Trump debates suggests that, in the social media sphere, the debate format or candidate pairing may exert less influence on polarization intensity than the broader partisan climate [@cantu2023]. This stability aligns with prior research arguing that debates’ persuasive effects are often limited [@olson2025], yet their symbolic role as moments of partisan reinforcement remains strong [@martinezabellan2024]. Put differently, debate talk on social media appears to function more as identity signaling and in-group affirmation than as persuasion, consistent with work on affective polarization as an out-group evaluation process [@IyengarSoodLelkes2012; @Wagner2021] and with research on partisan media and discussion dynamics in networked environments [@Levendusky2013; @LeeCho2023].

The slightly negative average sentiment (M = −0.111) and the association between higher polarization levels and more negative sentiment echo @bucy2020 observations about the affective and antagonistic tone of online debate reactions. The finding that High polarization tweets were significantly more negative than Low or Medium supports affective intelligence theory, which holds that heightened partisan conflict amplifies negative emotional responses [@marcus2000]. The sentiment gap between left- and right-leaning tweets (with left-leaning tweets being more negative) may reflect expectation gaps and perceived performance—patterns consistent with research on partisan media effects and candidate image repair [@franco-hantzsch2022; @eaton2024]. Normatively, this coupling of negativity and polarization raises familiar concerns about deliberative quality and trust[@MendelbergOleske2000; @TorcalMagalhaes2022], even as polarization can mobilize participation in some contexts [@HarteveldWagner2023].

A key methodological contribution of this study is the evaluation of large language models (LLMs) as annotators. The substantial disagreement between GPT-4 and Gemma3:12B (Krippendorff’s α = −0.466) reveals the risks of assuming cross-model consistency in content coding. The scatterplot analysis indicates divergences not only in intensity but also in directional classification, suggesting that prompt design and model architecture strongly influence annotation outcomes, an issue that mirrors broader concerns in computational social science regarding reproducibility and bias and the need for human validation in context [@mazorra2025]. In light of these findings, adoption of LLMs for political text annotation should incorporate cross-model validation and human-in-the-loop protocols, especially for ordinal and ideologically charged variables. While GPT-4 was chosen as the coder for training, the lack of convergence with Gemma3 points to a need for standardized evaluation frameworks, much like those proposed in multimodal political debate analysis [@shah2023], and to preserve and test the full ordinal information rather than early collapsing categories.

Two implications follow for interventions that track with your results. First, because perceptions of polarization are sensitive to information mixes, incidental exposure to counter-attitudinal content and network heterogeneity remain promising levers: both can reduce polarization under conditions of elaboration and political tolerance [@chen2022; @xia2023]. Second, hostile interpretations among partisans can heighten perceived polarization even without large changes in issue extremity; clarifying actual levels of disagreement and lowering out-group threat may help in high-salience moments [@TongWincklerRojas2021; @renstrom2021]. These points are consistent with comparative evidence that trajectories of affective polarization vary across countries and often track elite polarization [@BoxellGentzkowShapiro2022], and with calls to contextualize platform findings within broader political environments [@rojas2019; @SchermanEtAl2022; @ValenzuelaBachmannBargsted2021; @valenzuela2022; @rojas2025].

Finally, the study’s two-hour windows capture acute reactions rather than the full news cycle. This is a strength for isolating debate effects but leaves downstream reframing, meme diffusion, and cross-platform propagation outside the scope here. Extending the temporal window, adding basic coordination/bot screens, and integrating the multimodal dimensions highlighted by @bucy2020 and @shah2016 , linking textual sentiment and polarization with visual cues, candidate nonverbals, and meme trajectories, could clarify how affective polarization is co-produced across media layers and why certain debate moments achieve viral lifecycles that extend influence beyond the event itself.

# Conclusion

The findings allow clear answers to the RQs while underscoring the study’s substantive and methodological contributions. For our first research question ideological composition varied across events (left-leaning tweets = 60.4%), but polarization intensity was strikingly stable across the Biden–Trump and Harris–Trump debates: medium polarization predominated (51.9%). This pattern is consistent with a hybrid media environment in which debate talk operates less as persuasion than as identity signaling and in-group affirmation [@wells2016; @lukito2022; @IyengarSoodLelkes2012; @Levendusky2013; @LeeCho2023]. For our second research question average sentiment was slightly negative (M = −0.111), and negativity increased monotonically from Low to High polarization; left-leaning tweets were more negative on average. These results echo observations about the affective, antagonistic tone of online debate reaction and comport with affective-intelligence expectations that heightened partisan conflict amplifies negative emotion [@bucy2020; @marcus2000], with downstream implications for deliberation and democratic support even as mobilization can rise under affective polarization [@MendelbergOleske2000; @torcal2022; @HarteveldWagner2023]. For the third research question, cross-model agreement was poor: GPT-4 and Gemma3:12B produced a substantially negative Krippendorff’s α (−0.466), indicating systematic disagreement on both direction and intensity. For the final research question, GPT-4 provided the more consistent annotations under the proposed codebook, but the gap is bridgeable with targeted adaptation of open-weight models and stronger validation workflows.

These conclusions reinforce the overall importance of the measurement process advanced here. Substantively, the persistence of medium polarization alongside slight overall negativity suggests that social media debate reactions are best understood as performances of partisan identity within algorithmically curated attention markets, where a relatively small share of extreme content can still sustain perceptions of a polarized climate [@Sunstein2007; @Stroud2010; @YangEtAl2016; @wells2016; @lukito2022].

Methodologically, this study demonstrates both the promise and limitations of using Large Language Models (LLMs) as coders in political communication research. This work positions itself within the growing field of Computational Social Science, which leverages large-scale digital trace data to understand complex social phenomena like political polarization [@lazer2009]. The analysis is based on social-network-sourced big data from Twitter, a source known for its volume and unstructured nature, which necessitates the robust cleaning and classification pipeline developed here [@tan2013].

A core contribution of this project is its commitment to open science principles. The entire workflow is documented in a series of Jupyter Notebooks, providing a transparent and reproducible methodology. This approach aligns with the movement toward Open Behavioral Science and the use of computational notebooks to enhance scientific clarity and verification [@wagenmakers2017; @rule2018].

To ensure analytical rigor, the project applied inter-coder reliability testing in content analysis [@krippendorff2017] to the LLM annotation process. The substantial disagreement found between GPT-4 and the local Gemma model (Krippendorff’s α = -0.466) show the critical need for validation when using different AI architectures for nuanced classification tasks. Given its higher initial consistency, the GPT-4 labeled data was used as the ground truth for training the final BERT-LGBM classifier, which demonstrated superior performance in capturing the semantic complexities of political language.

Future work should explore whether larger or fine-tuned open-weight models, perhaps enhanced via Low-Rank Adaptation (LoRA), can improve accuracy and alignment with coding guidelines specially in local models suchs a Gemma3. The development of a robust coding protocol will require standardizing prompts and systematically testing reliability across multiple models. Without such refinements, LLM-based coders risk introducing measurement instability, undermining the very scales they are intended to operationalize. While proprietary models may currently offer stronger performance, advances in open-weight models and adaptation techniques hold the potential to reduce performance gaps, increase reproducibility, and ensure that the coding of political polarization is both rigorous and scalable.

# References
