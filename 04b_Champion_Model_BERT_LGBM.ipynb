{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f43409-19c1-4506-8415-feaa337593d5",
   "metadata": {},
   "source": [
    "**NOTEBOOK ON BERT CLASSIFIER + LGBM CLASSIFIER**\n",
    "\n",
    "This notebook is to see if a bert classifier help us to clean the data for a better classification  of polairzation.\n",
    "You will likely need to install these libraries first.\n",
    "In your terminal or a new notebook cell, run:\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "\n",
    "**BERT**\n",
    "\n",
    "BERT stands for: \"Bidirectional encoder representations from transformers (BERT)\", and basically is a Language Model with self-supervised learning. This will aloow us to wrok the Natural Language Processing (NLP) of our databae composed of tweets,ad processe them as vectors. This step will allos us not ontluy to manage in more interpetable way the text for an algorithm to work, but also to reduce the ambiguity for the algorithm. For BERT we are going to use a distilled version distilBERT.\n",
    "\n",
    "\"Fundamentally, BERT is a stack of Transformer encoder layers (Vaswani et al., 2017) which consist\n",
    "of multiple self-attention \"heads\". For every input token in a sequence, each head computes key, value\n",
    "and query vectors, used to create a weighted representation. The outputs of all heads in the same layer\n",
    "are combined and run through a fully-connected layer. Each layer is wrapped with a skip connection\n",
    "and followed by layer normalization.\n",
    "The conventional workflow for BERT consists of two stages: pre-training and fine-tuning. Pre-\n",
    "training uses two self-supervised tasks: masked language modeling (MLM, prediction of randomly\n",
    "masked input tokens) and next sentence prediction (NSP, predicting if two input sentences are adjacent\n",
    "to each other). In fine-tuning for downstream applications, one or more fully-connected layers are\n",
    "typically added on top of the final encoder layer.\n",
    "The input representations are computed as follows: each word in the input is first tokenized into\n",
    "wordpieces (Wu et al., 2016), and then three embedding layers (token, position, and segment) are\n",
    "combined to obtain a fixed-length vector. Special token [CLS] is used for classification predictions,\n",
    "and [SEP] separates input segments.\" - Rogers et al, 2020.\n",
    "\n",
    "**LGBM**\n",
    "\n",
    "Light GBM stands for \"Light Gradient-Boosting Machine\". Is a framework developed for microsoft to wrok with decistion tree algorithms. For algorithm, after try different set of algorithms, we are goping to use LGBM classifier. The abasic strucutre of a tree algorithm is a tree data structure. This are hierarchical and composed of nodes connected by edges, each node can have \"children\" nodes and one \"parent\" node, except for the root node which has no parent. \n",
    "\n",
    "\n",
    "\n",
    "The idea for a larger project would be to validate it with human coders.\n",
    "Load the small, training-labeled dataset\n",
    "This file contains the 'polar_score' needed for training.\n",
    "For this exercise we choose to use GPT capabilities to train the small dataset sample. \n",
    "The idea for a larger project would be to validate it with human coders.\n",
    "\n",
    "**Structure:**\n",
    "\n",
    "Setup and Imports: Imports libraries including torch, transformers (for DistilBertModel and tokenizer), lightgbm, and joblib for model saving.\n",
    "\n",
    "Data: The GPT-4 labeled data is loaded. A function, get_polarization_level, is defined to map the numerical polar_score into four distinct categories: None, Low, Medium, and High. This was made in this way beacuse in the normal scores we have some codes GPT didn't fill like -4,and some others were there were too few to work adequately in a training model. The data is split into training and testing sets based on this new 4-level target variable.\n",
    "\n",
    "BERT embedding generation: The pre-trained distilbert-base-uncased model and tokenizer are loaded. A get_bert_embeddings function is defined to process the santext column. It tokenizes the text and passes it through the BERT model to generate a 768-dimensional vector embedding for each tweet. This captures the semantic meaning of the text and since is vectorized it will help to reduce noise and give more clarity to the classifier. The nadi side of this apporach is that it require more computer power, so it is recommended to do it in the local hotspot of Anaconda or Gogle COLAB. In the case of this exercise was made in the local hotspot online of Anaconda and when try to work on the LAB the time and computer consumptios was noticeable slower.\n",
    "\n",
    "Model training and evaluation: An initial LogisticRegression model is trained on the BERT embeddings as a benchmark. The main LightGBM Classifier (LGBMClassifier) is trained using the BERT embeddings as input features. The model's performance is evaluated using a classification_report and a confusion matrix, showing its accuracy on the 4-level classification task.\n",
    "\n",
    "Production and saving: The trained LGBMClassifier is saved as the final \"champion model\" to a .pkl file using joblib. The final section of the notebook details the \"production\" process, where the large, unlabeled dataset is loaded in chunks. For each chunk, BERT embeddings are generated, and the saved champion model is used to predict the polarization level. The processed chunks are saved and later merged into a final, fully classified dataset.\n",
    "\n",
    "\n",
    "**References**\n",
    "\n",
    "Reference for Bert\n",
    "@article{devlin2018bert,\n",
    "  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n",
    "  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n",
    "  journal={arXiv preprint arXiv:1810.04805},\n",
    "  year={2018}\n",
    "}\n",
    "\n",
    "@misc{rogers2020primerbertologyknowbert,\n",
    "      title={A Primer in BERTology: What we know about how BERT works}, \n",
    "      author={Anna Rogers and Olga Kovaleva and Anna Rumshisky},\n",
    "      year={2020},\n",
    "      eprint={2002.12327},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL},\n",
    "      url={https://arxiv.org/abs/2002.12327}, \n",
    "}\n",
    "\n",
    "Reference for LGBM\n",
    "Yu Shi, Guolin Ke, Zhuoming Chen, Shuxin Zheng, Tie-Yan Liu. \"Quantized Training of Gradient Boosting Decision Trees\" (link). Advances in Neural Information Processing Systems 35 (NeurIPS 2022), pp. 18822-18833.\n",
    "\n",
    "Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. \"LightGBM: A Highly Efficient Gradient Boosting Decision Tree\". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.\n",
    "\n",
    "Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu. \"A Communication-Efficient Parallel Algorithm for Decision Tree\". Advances in Neural Information Processing Systems 29 (NIPS 2016), pp. 1279-1287.\n",
    "\n",
    "Huan Zhang, Si Si and Cho-Jui Hsieh. \"GPU Acceleration for Large-scale Tree Boosting\". SysML Conference, 2018.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd0e6c-2dc8-4682-9afa-443967867892",
   "metadata": {},
   "source": [
    "**MODEL 6: BERT + LightGBM evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3677f34-26be-44ff-afcc-a13050b6a46c",
   "metadata": {},
   "source": [
    "**MODEL 6: BERT + LightGBM EVALUATION**\n",
    "\n",
    "This part of the code is just to load the data and evaluate the statistics of the combination of the prcoes and a generate a complete model. If the statistics are beter than the other models this will be the final (champion) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0360bd8f-9f27-475e-b960-53ec19059069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  libraries\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import os\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# We will use the same data splits from our previous experiment\n",
    "# X_train_level, X_test_level, y_train_level, y_test_level\n",
    "# and the same category order\n",
    "# category_order = ['None', 'Low', 'Medium', 'High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba0927d-0278-46ae-a339-87b4597e8bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded and prepared for the BERT model.\n",
      "Training features (X_train_level) shape: (726,)\n",
      "Testing features (X_test_level) shape: (182,)\n",
      "\n",
      "Value counts in the training labels (y_train_level):\n",
      "polarization_level\n",
      "Medium    331\n",
      "Low       301\n",
      "High       61\n",
      "None       33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# parte 2 la data\n",
    "# Load the small, training-labeled dataset\n",
    "# This file contains the 'polar_score' needed for training.\n",
    "# For this exercise we choose to use GPT capabilities to train the small dataset sample. \n",
    "#The idea for a larger project would be to validate it with human coders.\n",
    "labeled_filepath = \"data/sample/debate_tweets_sample_1000_for_annotation_labeled.csv\" #this is the final csv from notebook 2\n",
    "df_labeled = pd.read_csv(labeled_filepath)\n",
    "\n",
    "\n",
    "# Clean and Prepare Data\n",
    "# Filter out invalid scores and convert the column to a numeric type\n",
    "df_multi = df_labeled[df_labeled['polar_score'] != 'INV'].copy()\n",
    "df_multi['polar_score'] = pd.to_numeric(df_multi['polar_score'])\n",
    "\n",
    "\n",
    "# Define the function to create the 4 polarization levels\n",
    "def get_polarization_level(score):\n",
    "    score = int(score)\n",
    "    if score in [-7, -6, 6, 7]:\n",
    "        return 'High'\n",
    "    elif score in [-5, -4, 4, 5]:\n",
    "        return 'Medium'\n",
    "    elif score in [-3, -2, 2, 3]:\n",
    "        return 'Low'\n",
    "    elif score in [-1, 0, 1]:\n",
    "        return 'None'\n",
    "    return None # Fallback\n",
    "\n",
    "# Create the new target column by applying the function\n",
    "df_multi['polarization_level'] = df_multi['polar_score'].apply(get_polarization_level)\n",
    "\n",
    "\n",
    "# Create the final training and testing splits with the variables BERT will need.\n",
    "X_train_level, X_test_level, y_train_level, y_test_level = train_test_split(\n",
    "    df_multi['santext'].fillna(''),  # Use .fillna('') as a safeguard\n",
    "    df_multi['polarization_level'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_multi['polarization_level']\n",
    ")\n",
    "\n",
    "# Confirmation Print\n",
    "print(\"Data successfully loaded and prepared for the BERT model.\")\n",
    "print(f\"Training features (X_train_level) shape: {X_train_level.shape}\")\n",
    "print(f\"Testing features (X_test_level) shape: {X_test_level.shape}\")\n",
    "print(\"\\nValue counts in the training labels (y_train_level):\")\n",
    "print(y_train_level.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a908622-52a5-4dee-a435-119def6bc6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 726 texts...\n",
      "  Processed 160 / 726\n",
      "  Processed 320 / 726\n",
      "  Processed 480 / 726\n",
      "  Processed 640 / 726\n",
      "Embedding generation complete.\n",
      "Generating embeddings for 182 texts...\n",
      "  Processed 160 / 182\n",
      "Embedding generation complete.\n",
      "\n",
      "Shape of training features: (726, 768)\n",
      "Shape of testing features: (182, 768)\n"
     ]
    }
   ],
   "source": [
    "#parte 3 la función\n",
    "# Load the pre-trained DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# This part it depends of the equimpemt with the development of the project.\n",
    "# This was imnportnat to add since the original project was running on a IOS macbook.\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=32):\n",
    "    \n",
    "    all_embeddings = []\n",
    "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "    \n",
    "    # Process texts in batches to manage memory\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize the batch of texts\n",
    "        inputs = tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        \n",
    "        # The embedding for each sentence is the hidden state of the [CLS] token\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.extend(cls_embeddings)\n",
    "        \n",
    "        if (i + batch_size) % (batch_size * 5) == 0:\n",
    "             print(f\"  Processed {i + batch_size} / {len(texts)}\")\n",
    "\n",
    "    print(\"Embedding generation complete.\")\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Generate embeddings for our training and testing sets\n",
    "# We convert the pandas Series to a list for processing\n",
    "X_train_bert = get_bert_embeddings(X_train_level.tolist())\n",
    "X_test_bert = get_bert_embeddings(X_test_level.tolist())\n",
    "\n",
    "print(f\"\\nShape of training features: {X_train_bert.shape}\")\n",
    "print(f\"Shape of testing features: {X_test_bert.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa371221-ce75-4b3f-a65f-000bc3208213",
   "metadata": {},
   "source": [
    "Modelo, training and evaluaitng the clasdsifier con bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a22bf24-eb6e-404f-a4a0-7ad97d34150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT + Logistic Regression Evaluation (Polarization Level) \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       1.00      0.25      0.40         8\n",
      "         Low       0.59      0.63      0.61        76\n",
      "      Medium       0.60      0.63      0.62        83\n",
      "        High       0.67      0.53      0.59        15\n",
      "\n",
      "    accuracy                           0.60       182\n",
      "   macro avg       0.71      0.51      0.55       182\n",
      "weighted avg       0.62      0.60      0.60       182\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAIhCAYAAADejQtoAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAYiNJREFUeJzt3X98zfX///H72Wxnm83Mr82E5vfv3+VXsSk/SyQlVERFSiSRyEYYq1D5lfIr+fmOvOudQkK0hPxIfsdQMSPDDMP2+v7hs/Pt2LBpZ69jr9vV5XW5OM/X87xej/M6O/PweL6ez2MzDMMQAAAALMPD7AAAAACQu0gAAQAALIYEEAAAwGJIAAEAACyGBBAAAMBiSAABAAAshgQQAADAYkgAAQAALIYEEAAAwGJIAPOY2bNny2azOW1FixZVeHi4/ve//2Xof33ff27du3d39IuKinLa5+XlpVKlSun5559XfHy8JCk8PPymx0vfoqKicuVapMd86tSpXDlfurvvvtvp2mVFbGysoqKidObMmQz7wsPDFR4eniOxde/e3em98Pb2VtmyZTVw4ECdO3cuR85xJ8jJa3o7Tpw4oTfeeEPVq1eXv7+/fHx8VL58efXr108HDhxw6blPnz6tJ598UsWKFZPNZlP79u1z/BxmXd/Dhw/f8vdMjx49HH1ux/Lly2/rd1hu/u4DsiKf2QHANWbNmqVKlSrJMAzFx8dr0qRJatu2rb788ku1bdvWqW/Hjh312muvZThG0aJFM7R9++23CgwM1Pnz57Vy5Uq99957io2N1fbt2zVlyhSnJOLrr7/WqFGjHLGku+uuu3LwlbqfL774QgUKFMjWc2JjYzVixAh1795dBQsWdNo3ZcqUHIxO8vX11ffffy9JOnPmjD7//HO99957+vXXX7Vy5cocPZe7yulrmh2bNm3Sww8/LMMw9PLLL6thw4by9vbWvn379Nlnn+nee+9VYmKiy87/9ttv64svvtDMmTNVtmxZFSpUKMfPYeb1laSAgADNnj1bw4cPl4fH/69znD9/Xv/5z39UoECB2/4Pz/LlyzV58uRsJ3M//fRTnv/dhzsLCWAeVa1aNdWrV8/xuFWrVgoKCtKCBQsyJIDBwcFq0KBBlo5bt25dFSlSRJL04IMP6tSpU5o1a5Y2bNigiIgIp7579+7NNJbsstlsmjVrVraramapXbt2jh6vSpUqOXo8Dw8Pp/e7VatWOnTokFatWqW4uDiFhYXl6PluJjU1VVevXpXdbs+1c0o5f02z6ty5c2rXrp18fHwUGxvrlBCEh4erV69e+vzzz10aw2+//aayZcuqa9euLjuHWdc3XadOnfTJJ59o9erVat68uaN90aJFSk1NVfv27fXZZ5+5PA7DMHTp0iX5+vpm+XcskFsYArYIHx8feXt7y8vLK0ePm57YnThxIkePm5u+/PJLNWzYUH5+fgoICFDz5s31008/Zej33//+VzVq1JDdbleZMmX0/vvvO4aZ/+n6IeC0tDSNGjVKFStWlK+vrwoWLKgaNWro/fffl3RtqPr111+XJIWFhTmGp9auXSsp8+G0lJQUjRw5UpUrV5aPj48KFy6siIgIxcbG3tY1uNH7uGjRIjVs2FD58+eXv7+/WrZsqW3btmV4/scff6wKFSrIbrerSpUqmj9/vrp37667777b0Sd9eC4mJkajRo1SWFiY7Ha71qxZI0nasmWLHnnkERUqVEg+Pj6qXbu2Fi9e7HSeCxcuaODAgQoLC5OPj48KFSqkevXqacGCBY4+hw4d0pNPPqnQ0FDZ7XYFBwfrgQce0Pbt2x19Mrump0+fVp8+fVSiRAl5e3urTJkyGjp0qFJSUpz62Ww2vfzyy5o7d64qV64sPz8/1axZM9NbLDK7TvHx8YqJiblhNahjx45Oj7Py85n+c7hr1y517txZgYGBCg4OVo8ePXT27Fmn6//dd99pz549Tj9na9eudfqZS5f+nNmzZ98R1zddxYoV1ahRI82cOdOpfebMmerQoYMCAwMzPGfRokVq0aKFihcvLl9fX1WuXFlvvPGGkpOTHX26d++uyZMnO+JM3w4fPuwU+7Rp01S5cmXZ7XbNmTPHsS+9amgYhtq0aaPChQvr6NGjjuNfuHBBVatWVeXKlZ3OC7gCFcA8Kr2yYhiGTpw4oXfeeUfJycnq0qVLhr6GYejq1asZ2j09PW95n0xcXJwkqUKFCjkTeC6bP3++unbtqhYtWmjBggVKSUlRTEyMwsPDtXr1at13332Srg19d+jQQU2aNNGiRYt09epVvfvuu1lKfGNiYhQVFaVhw4apSZMmunLlivbu3eu43++5557T6dOn9eGHH2rp0qUqXry4pBtXUa5evarWrVtr/fr16t+/v5o1a6arV69q48aNOnr0qBo1apTt6xAXF6d8+fKpTJkyjrYxY8Zo2LBhevbZZzVs2DBdvnxZ77zzju6//35t2rTJEd/06dPVq1cvPfbYY5owYYLOnj2rESNGZPiHPd0HH3ygChUq6N1331WBAgVUvnx5rVmzRq1atVL9+vU1bdo0BQYGauHCherUqZMuXLjgSKgHDBiguXPnatSoUapdu7aSk5P122+/6e+//3Ycv02bNkpNTVVMTIxKlSqlU6dOKTY2NtP7K9NdunRJEREROnjwoEaMGKEaNWpo/fr1io6O1vbt2/X111879f/666+1efNmjRw5Uv7+/oqJidGjjz6qffv2OV3D661cuVKenp4ZqvA3ktWfz3SPPfaYOnXqpJ49e2rnzp0aMmSIpGuJT/HixfXTTz+pT58+Onv2rObNmyfp2s/Z1q1bsxSP5N7X95969uypl156SYmJiQoKCtK+ffsUGxurUaNGacmSJRn6HzhwQG3atFH//v2VP39+7d27V+PGjdOmTZsct0y89dZbSk5O1ueff+6UhKd/ZiVp2bJlWr9+vYYPH66QkBAVK1Ysw7lsNpvmzp2rWrVq6YknntD69evl5eWlPn36KC4uTj///LPy58+fpdcJ3DYDecqsWbMMSRk2u91uTJkyJUP/zPqmb3PnznX0i4yMNCQZ8fHxxpUrV4zExERj8eLFRv78+Y3OnTvfNJbNmzdnOf7U1FTjypUrTpskY8aMGU5tV69eveWx0mM+efLkDc8VGhpqVK9e3UhNTXW0JyUlGcWKFTMaNWrkaLvnnnuMkiVLGikpKU79ChcubFz/MSpdurTRrVs3x+OHH37YqFWr1k1jfeeddwxJRlxcXIZ9TZs2NZo2bep4/OmnnxqSjI8//vimx8xMt27djPz58zuu46lTp4ypU6caHh4exptvvunod/ToUSNfvnxG3759nZ6flJRkhISEGE888YRhGNeuYUhIiFG/fn2nfkeOHDG8vLyM0qVLO9ri4uIMSUbZsmWNy5cvO/WvVKmSUbt2bePKlStO7Q8//LBRvHhxx/tTrVo1o3379jd8fadOnTIkGRMnTrzpdbj+mk6bNs2QZCxevNip37hx4wxJxsqVKx1tkozg4GDj3Llzjrb4+HjDw8PDiI6Ovul5K1WqZISEhNy0T7rs/Hym/6zHxMQ4HaNPnz6Gj4+PkZaW5vTaq1at6tRvzZo1hiRjzZo1Tu3p79msWbMMw3D/65se7zvvvGMkJSUZ/v7+xqRJkwzDMIzXX3/dCAsLM9LS0oyXXnopw+f2n9LS0owrV64Y69atMyQZO3bscOy72XMlGYGBgcbp06cz3RcZGenUtmHDBiNfvnxG//79jZkzZxqSjE8++eSmrxHIKQwB51GffvqpNm/erM2bN+ubb75Rt27d9NJLL2nSpEkZ+j7xxBOOvv/c2rRpk6FvSEiIvLy8FBQUpCeeeEJ169Z1DHHkhJEjR8rLy8tpk679b/6fbWXLlv3X59q3b5+OHTump59+2ulGcX9/fz322GPauHGjLly4oOTkZG3ZskXt27eXt7e3U7+sVHLuvfde7dixQ3369NGKFSv+9Wzbb775Rj4+PurRo8dtPT85OdlxHYsUKaIXX3xRnTp10ujRox19VqxYoatXr+qZZ57R1atXHZuPj4+aNm3qGCrct2+f4uPj9cQTTzido1SpUmrcuHGm53/kkUecbkX4/ffftXfvXsc9af88X5s2bXT8+HHt27dP0rVr+c033+iNN97Q2rVrdfHiRadjFypUSGXLltU777yj8ePHa9u2bUpLS7vlNfn++++VP3/+DMOv6ZXH1atXO7VHREQoICDA8Tg4OFjFihXTkSNHbnmurMrqz+c/PfLII06Pa9SooUuXLikhISFHYrqTrq+/v78ef/xxzZw5U1evXtWnn36qZ5999oajGocOHVKXLl0UEhIiT09PeXl5qWnTppKkPXv2ZPm8zZo1U1BQUJb6Nm7cWKNHj9bEiRP14osv6qmnnlLPnj2zfC7g3yABzKMqV66sevXqqV69emrVqpU++ugjtWjRQoMGDcowVFO0aFFH339umc0O/O6777R582atWLFCjz32mH744Qf17ds3x+J+4YUXMiSikhQZGenU9tVXX/3rc6UPG/5z+CZdaGio0tLSlJiYqMTERBmGoeDg4Az9Mmu73pAhQ/Tuu+9q48aNat26tQoXLqwHHnhAW7Zsua24T548qdDQUKekIDt8fX2drmN4eLgWLFigsWPHOvqkD23fc889GRLyRYsWOZbWSb+G2bk211/v9HMNHDgww7n69OkjSY7zffDBBxo8eLCWLVumiIgIFSpUSO3bt3csnWKz2bR69Wq1bNlSMTExqlOnjooWLapXXnlFSUlJN7wmf//9t0JCQjIkB8WKFVO+fPmchpglqXDhwhmOYbfbMySk1ytVqpROnjyZpfu7svrzebO40ifX3CqurHL363u9nj17auvWrRo9erROnjx5w4lk58+f1/3336+ff/5Zo0aN0tq1a7V582YtXbpUUvauX2bv18107dpV3t7eSklJcdwLDOQG7gG0kBo1amjFihXav3+/7r333ts6Rs2aNR2zgJs3b66WLVtq+vTp6tmzp+65555/HWNoaKhCQ0MztN99993/aiZxZtL/kTl+/HiGfceOHZOHh4eCgoJkGIZsNlum9/ulr4F4M/ny5dOAAQM0YMAAnTlzRt99953efPNNtWzZUn/88Yf8/PyyFXfRokW1YcMGpaWl3VYS6OHh4XQtmzdvrrp162rEiBHq2rWrSpYs6XiPP//8c5UuXfqGx0q/htm5NtcnAennGjJkiDp06JDpcypWrChJyp8/v0aMGKERI0boxIkTjmpg27ZtHbPOS5curRkzZkiS9u/fr8WLFysqKkqXL1/WtGnTbvg6fv75Z8d7nS4hIUFXr151xPhvtWzZUitXrtRXX32lJ5988qZ9s/rzmRN8fHwkKcN9m5mtoenO1/d6jRs3VsWKFTVy5Eg1b95cJUuWzLTf999/r2PHjmnt2rWOqp+km97XeCPZWV8wNTVVXbt2VVBQkOx2u3r27Kkff/zRaaQBcBUqgBaSPksvs/X9bofNZtPkyZPl6empYcOG5cgxc1PFihVVokQJzZ8/X4ZhONqTk5O1ZMkSx8zL/Pnzq169elq2bJkuX77s6Hf+/PlszUyUpIIFC6pjx4566aWXdPr0acfswexUalq3bq1Lly45zcz8N+x2uyZPnqxLly5p1KhRkq4lKvny5dPBgwczrQ6nJ5AVK1ZUSEhIhtm6R48ezfKM5IoVK6p8+fLasWPHDc/1z+HAdMHBwerevbs6d+6sffv2ZRgOla5NTho2bJiqV69+04kODzzwgM6fP69ly5Y5tX/66aeO/TmhZ8+eCgkJ0aBBg/TXX39l2ie96pTVn8+ckD5b+9dff3Vq//LLL2/6PHe7vpkZNmyY2rZtm+lap+nSk7brlyP66KOPMvTNyapqZGSk1q9fr3nz5mnRokXasWMHVUDkGiqAedRvv/3mmNn7999/a+nSpVq1apUeffTRDOu8nThxQhs3bsxwjAIFCtxyPa/y5cvrhRde0JQpU7Rhw4YMsxLdwVdffZVpAtGxY0fFxMSoa9euevjhh9WrVy+lpKTonXfe0ZkzZ5yGREeOHKmHHnpILVu2VL9+/ZSamqp33nlH/v7+On369E3P37ZtW8daiEWLFtWRI0c0ceJElS5dWuXLl5ckVa9eXZL0/vvvq1u3bvLy8lLFihUzjbtz586aNWuWevfurX379ikiIkJpaWn6+eefVbly5VtWljLTtGlTtWnTRrNmzdIbb7yhsLAwjRw5UkOHDtWhQ4cc60ieOHFCmzZtclTiPDw8NGLECPXq1UsdO3ZUjx49dObMGY0YMULFixfPcoXyo48+UuvWrdWyZUt1795dJUqU0OnTp7Vnzx5t3bpV//nPfyRJ9evX18MPP6waNWooKChIe/bs0dy5cx3J0K+//qqXX35Zjz/+uMqXLy9vb299//33+vXXX/XGG2/c8PzPPPOMJk+erG7duunw4cOqXr26NmzYoDFjxqhNmzZ68MEHs31NMxMYGKj//ve/evjhh1W7dm2nhaAPHDigzz77TDt27FCHDh3k4eGR5Z/PfyskJEQPPvigoqOjFRQUpNKlS2v16tWOZDSdu1/fzDz11FN66qmnbtqnUaNGCgoKUu/evRUZGSkvLy/NmzdPO3bsyNA3/bM6btw4tW7dWp6enqpRo0a2q3arVq1SdHS03nrrLUcCHB0drYEDByo8PFyPPvpoto4HZJuZM1CQ8zKbBRwYGGjUqlXLGD9+vHHp0iWn/tf3/efWuHFjR7+bzag9ceKE4e/vb0RERGQaS3ZmAWdG/5iFmB3pMd9oS7ds2TKjfv36ho+Pj5E/f37jgQceMH788ccMx/viiy+M6tWrG97e3kapUqWMsWPHGq+88ooRFBTk1O/6WcDvvfee0ahRI6NIkSKO5/bs2dM4fPiw0/OGDBlihIaGGh4eHk4zMq+fUWkYhnHx4kVj+PDhRvny5Q1vb2+jcOHCRrNmzYzY2NibXpP0WcCZ2blzp+Hh4WE8++yzTtcmIiLCKFCggGG3243SpUsbHTt2NL777jun506fPt0oV66c4e3tbVSoUMGYOXOm0a5dO6N27dqOPv+coZmZHTt2GE888YRRrFgxw8vLywgJCTGaNWtmTJs2zdHnjTfeMOrVq2cEBQUZdrvdKFOmjPHqq68ap06dMgzj2s9i9+7djUqVKhn58+c3/P39jRo1ahgTJkxwmjme2TX9+++/jd69exvFixc38uXLZ5QuXdoYMmRIpp+Zl156KUP817/vNxMfH28MHjzYqFq1quHn52fY7XajXLlyRq9evYydO3c69c3Kz+eNPp/pn8F/zi7PbBawYRjG8ePHjY4dOxqFChUyAgMDjaeeesrYsmWL0+fP3a/vrX7G0mU2kzc2NtZo2LCh4efnZxQtWtR47rnnjK1bt2b4/ZOSkmI899xzRtGiRQ2bzeZ0fW8Ue/q+9FnAx44dM4oVK2Y0a9bMaYZ3Wlqa0bZtW6NgwYKZrggA5CSbYfxjbAFAll25ckW1atVSiRIlLPMVall15swZVahQQe3bt9f06dPNDgcAcB2GgIEs6tmzp5o3b67ixYsrPj5e06ZN0549exzf6GFV8fHxGj16tCIiIlS4cGEdOXJEEyZMUFJSkvr162d2eACATJAAAlmUlJSkgQMH6uTJk/Ly8lKdOnW0fPlyl96/dCew2+06fPiw+vTpo9OnT8vPz08NGjTQtGnTVLVqVbPDAwBkgiFgAAAAi2EZGAAAAIshAQQAALAYEkAAAACLIQEEAACwmDw5C/hK2jazQ0AuSjVSbt0JeYanzX7rTsgzPG18L66VeNjMWznAt1Rnlx374tEFLjv27aICCAAAYDF5sgIIAACQHTabtWpiJIAAAMDybBYbFLXWqwUAAAAVQAAAAKsNAVvr1QIAAIAKIAAAABVAAAAA5GlUAAEAgOXZbDazQ8hVVAABAAAshgogAACAxWpiJIAAAMDymAQCAACAPI0EEAAAWJ7N5uGyLTuioqJks9mctpCQEMd+wzAUFRWl0NBQ+fr6Kjw8XLt27cr26yUBBAAAcCNVq1bV8ePHHdvOnTsd+2JiYjR+/HhNmjRJmzdvVkhIiJo3b66kpKRsnYN7AAEAgOXZXFgTS0lJUUpKilOb3W6X3W7PtH++fPmcqn7pDMPQxIkTNXToUHXo0EGSNGfOHAUHB2v+/Pnq1atXlmOiAggAAOBC0dHRCgwMdNqio6Nv2P/AgQMKDQ1VWFiYnnzySR06dEiSFBcXp/j4eLVo0cLR1263q2nTpoqNjc1WTFQAAQCA5blyFvCQIUM0YMAAp7YbVf/q16+vTz/9VBUqVNCJEyc0atQoNWrUSLt27VJ8fLwkKTg42Ok5wcHBOnLkSLZiIgEEAABwoZsN916vdevWjr9Xr15dDRs2VNmyZTVnzhw1aNBAUsZvLTEMI9vfZMIQMAAAsDx3mQV8vfz586t69eo6cOCA477A9EpguoSEhAxVwVshAQQAAJbnrglgSkqK9uzZo+LFiyssLEwhISFatWqVY//ly5e1bt06NWrUKFvHZQgYAADATQwcOFBt27ZVqVKllJCQoFGjRuncuXPq1q2bbDab+vfvrzFjxqh8+fIqX768xowZIz8/P3Xp0iVb5yEBBAAAlmdT9u6hc5U///xTnTt31qlTp1S0aFE1aNBAGzduVOnSpSVJgwYN0sWLF9WnTx8lJiaqfv36WrlypQICArJ1HpthGIYrXoCZrqRtMzsE5KJUI+XWnZBneNqydiM18gZPm7fZISAXediqmnbuohVfddmxT+6b4LJj3y4qgAAAwPJcuQyMO7LWqwUAAAAVQAAAACqAAAAAyNOoAAIAAMuzWgWQBBAAAMBig6LWerUAAACgAggAAGC1IWBrvVoAAABQAQQAAKACCAAAgDyNCiAAALA8m8VqYtZ6tQAAAKACCAAAYLV7AEkAAQCA5dlsNrNDyFXWSncBAABABRAAAMBqQ8DWerUAAABwnwTw6tWr+u677/TRRx8pKSlJknTs2DGdP3/e5MgAAEBeZ5OHyzZ35BZDwEeOHFGrVq109OhRpaSkqHnz5goICFBMTIwuXbqkadOmmR0iAABAnuEWaWm/fv1Ur149JSYmytfX19H+6KOPavXq1SZGBgAArMBm83DZ5o7cogK4YcMG/fjjj/L29nZqL126tP766y+TogIAAMib3CIBTEtLU2pqaob2P//8UwEBASZEBAAArMRdK3Wu4havtnnz5po4caLjsc1m0/nz5xUZGak2bdqYFxgAALAEJoGYYMKECYqIiFCVKlV06dIldenSRQcOHFCRIkW0YMECs8MDAADIU9wiAQwNDdX27du1YMECbd26VWlpaerZs6e6du3qNCkEAADAJSw2BGwzDMMwO4icdiVtm9khIBelGilmh4Bc5Gmzmx0CcpGnzfvWnZBneNiqmnbuMnXGu+zYh7YOcNmxb5dbVAAlaf/+/Vq7dq0SEhKUlpbmtG/48OEmReX+Pp6+TN+t2qS4Q8fk4+OtWrUr6NXXuigsLNTs0OACixeu1uKF3+vYX6ckSWXLlVCvF9vpviY1TY4MrsDn21o2b96lmTP+q127DurkyUR9OGmwHnywvtlhWYbVJoG4RQL48ccf68UXX1SRIkUUEhIim83m2Gez2UgAb2LL5j3q3KWFqlUrq6upafpg4kK90HOM/vu/d+Xn52N2eMhhxYILqd+rT6hk6WBJ0lfLNqjfy+9r0ZKRKlf+LpOjQ07j820tFy+mqGKlu/Voh2bq90qM2eEgj3OLIeDSpUurT58+Gjx4cI4cz8pDwKdPn1OTxi9o9qeRqndPZbPDyRVWHwK+v0Efvfp6J3V4rKnZoeQKKw8BW/HzbdUh4MqVOliyAmjmEHC5eu+77Ni/b+nnsmPfLreoACYmJurxxx83O4w84XzSBUlSYKC/yZHA1VJT07RyxSZdvJiimjXLmR0OcgGfbwA5xS0SwMcff1wrV65U7969s/3clJQUpaQ4V4A8vC7Lbrfe/xoNw1DMuLmqU7eiylcoaXY4cJED+//Q053f1uXLV+Tn56MJH7yisuVKmB0WXIzPN+Ba7rpen6u4RQJYrlw5vfXWW9q4caOqV68uLy8vp/2vvPLKDZ8bHR2tESNGOLUNG/6ChkdmP5m8041+e5b27zuiT+eNuHVn3LHuvru4Fi99W0lJF/Tdys16682PNWPOEJLAPI7PN+BaVpsE4hb3AIaFhd1wn81m06FDh264P/MK4B7LVQDHjJql1as3a87cKN11VzGzw8lVVr8H8IUe43RXyWIaPuJZs0PJFVa8B9DKn2/uAeQewNxS4Z7JLjv2/s0vuezYt8stKoBxcXG3/Vy73S673fkfhCtp1vmFYRjGtX8cvtusWXOGW+4fB0iGIV25ctXsMOACfL6BXPSPFUiswC0SwH9KL0jaLPZG3K5RI2dq+dc/6oNJA5U/v69OnTwjSfIP8JOPj3USYav4YMJ/dN/9NRRcvJAuJF/St8t/1pbNezRl+kCzQ4ML8Pm2luTkizp6NN7x+M8/E7RnT5wCA/0VGlrUxMiQF7nFELAkffrpp3rnnXd04MABSVKFChX0+uuv6+mnn872say0DEy1yk9m2j5qTG+1fzQ8V2Mxi5WGgCOHzdCmjbt18uQZ+Qf4qkKFknr2uYfUsFE1s0PLNVYaAubzba0h4E0//6Zu3TKue9u+fYSix/Y1IaLcZ+oQcIMpLjv2/o19XHbs2+UWCeD48eP11ltv6eWXX1bjxo1lGIZ+/PFHTZ48WaNGjdKrr76areNZKQGEtRJAWCsBhLUSQJAA5ia3GAL+8MMPNXXqVD3zzDOOtnbt2qlq1aqKiorKdgIIAACQLRa79cwt5jwfP35cjRo1ytDeqFEjHT9+3ISIAAAA8i63SADLlSunxYsXZ2hftGiRypcvb0JEAADAUmw2121uyC2GgEeMGKFOnTrphx9+UOPGjWWz2bRhwwatXr0608QQAAAgR7lFSSz3uMXLfeyxx/Tzzz+rcOHCWrZsmZYuXaoiRYpo06ZNevTRR80ODwAAIE9xiwqgJNWtW1fz5s0zOwwAAGBBhpsO1bqKqQmgh4fHLRd8ttlsunqVbzkAAADIKaYmgF988cUN98XGxurDDz+UGyxTCAAA8jprFQDNTQDbtWuXoW3v3r0aMmSIvvrqK3Xt2lVvv/22CZEBAADkXW4xCUSSjh07pueff141atTQ1atXtX37ds2ZM0elSpUyOzQAAJDXedhct7kh0xPAs2fPavDgwSpXrpx27dql1atX66uvvlK1atb5blMAAIDcZOoQcExMjMaNG6eQkBAtWLAg0yFhAAAAl7PYLGCbYeIsCw8PD/n6+urBBx+Up6fnDfstXbo0W8e9krbt34aGO0iqkWJ2CMhFnja72SEgF3navM0OAbnIw1bVtHOXj/jYZcc+sOZ5lx37dplaAXzmmWduuQwMAACAy1ksHTE1AZw9e7aZpwcAALjGTSdruIrpk0AAAACQu9zmq+AAAABMY7Fb0qgAAgAAWAwVQAAAAGsVAKkAAgAAWA0VQAAAAGYBAwAAIC+jAggAAGCtAiAJIAAAgMEyMAAAAMjLqAACAAAwCQQAAAB5GRVAAAAAaxUAqQACAABYDRVAAAAAZgEDAAAgL6MCCAAAYLFZwCSAAAAA1sr/GAIGAACwGiqAAAAATAIBAABAXkYFEAAAgAogAAAA8jIqgAAAABYriVns5QIAANw5oqOjZbPZ1L9/f0ebYRiKiopSaGiofH19FR4erl27dmXruCSAAAAANpvrttu0efNmTZ8+XTVq1HBqj4mJ0fjx4zVp0iRt3rxZISEhat68uZKSkrJ8bBJAAAAAmwu323D+/Hl17dpVH3/8sYKCghzthmFo4sSJGjp0qDp06KBq1appzpw5unDhgubPn5/l45MAAgAAuFBKSorOnTvntKWkpNz0OS+99JIeeughPfjgg07tcXFxio+PV4sWLRxtdrtdTZs2VWxsbJZjIgEEAACWZ3jYXLZFR0crMDDQaYuOjr5hLAsXLtTWrVsz7RMfHy9JCg4OdmoPDg527MsKZgEDAAC40JAhQzRgwACnNrvdnmnfP/74Q/369dPKlSvl4+Nzw2Parru30DCMDG03QwIIAADgwoWg7Xb7DRO+6/3yyy9KSEhQ3bp1HW2pqan64YcfNGnSJO3bt0/StUpg8eLFHX0SEhIyVAVvhiFgAAAAN/HAAw9o586d2r59u2OrV6+eunbtqu3bt6tMmTIKCQnRqlWrHM+5fPmy1q1bp0aNGmX5PFQAAQAA3OSb4AICAlStWjWntvz586tw4cKO9v79+2vMmDEqX768ypcvrzFjxsjPz09dunTJ8nlIAAEAAO4ggwYN0sWLF9WnTx8lJiaqfv36WrlypQICArJ8DJthGIYLYzTFlbRtZoeAXJRq3HwqPfIWT1vW7qNB3uBp8zY7BOQiD1tV085d9umFLjv2wblPuuzYt4sKIAAAgAsngbgjJoEAAABYTJ6sAHp55Dc7BOSi0tWWmh0CclHFmFZmh4BctJK321I8zCzCWasASAUQAADAavJkBRAAACBbTC0/5j4qgAAAABZDBRAAAIAKIAAAAPIyKoAAAMDyDGsVAEkAAQAAGAIGAABAnkYFEAAAgK+CAwAAQF5GBRAAAIB7AAEAAJCXUQEEAACwWEnMYi8XAAAAVAABAAAsNguYBBAAAIBJIAAAAMjLqAACAADLMyw2BEwFEAAAwGKoAAIAAFisJGaxlwsAAAAqgAAAAMwCBgAAQF5GBRAAAMBis4BJAAEAABgCBgAAQF5GBRAAAMBaBUAqgAAAAFZDBRAAAFiewT2AAAAAyMuoAAIAAFABBAAAQF5GBRAAAMBiC0FTAQQAALAYKoAAAAAWK4mRAAIAADAEDAAAgLyMCiAAAADLwAAAACAvowIIAABABRAAAAB5GRVAAABgeQazgAEAAJCXUQEEAACwWEmMBBAAAIAhYAAAAORlVAABAABYBib3de3aVdOnT9f+/fvNDgUAACDPc4sE0N/fX+PHj1elSpUUGhqqzp07a9q0adq7d6/ZoQEAACvwsLluc0NukQB+9NFH2rt3r44dO6bx48crMDBQ77//vqpWrarixYubHR4AAECe4lb3AAYEBCgoKEhBQUEqWLCg8uXLp5CQELPDAgAAeZ17Fupcxi0qgIMHD1aDBg1UpEgRDRs2TJcvX9aQIUN04sQJbdu2zezwAAAA8hS3qAC+8847Klq0qCIjI9WuXTtVrlzZ7JDuOPPmfa0ZM5bq5MlElS9fSm+++bzq1atqdljIQS8/10Rv9m+uj+fGKnLcN5IkP19vDX21uVo2q6yggn7689gZzZj3kz5dtNnkaJFdXcqW0P3BhVXK308pqanalZik6fuO6I/ki079SuX31QuV7lbNQgXkYbPpcNIFjdi2VwmXLpsUOXLKx9OX6btVmxR36Jh8fLxVq3YFvfpaF4WFhZodmiUYbnqvnqu4RQK4bds2rVu3TmvXrtV7770nT09PNW3aVOHh4QoPDychvIXly9crOvoTRUb2Vp06VbRw4bd6/vkoff31ZIWGFjM7POSAmtVK6KmO9bRrX7xT+4jBrdXo3jD1HfK5/vjrjJo2KqfoYQ/rREKSVqxhEtWdpGahQC07Eq99Z5PkabOpZ8XSirm3ip79YZsupaZJkkL9fPRBw+r65o8Tmn3gqJKvXFVpfz9dTjNMjh45YcvmPercpYWqVSurq6lp+mDiQr3Qc4z++7935efnY3Z4eR8LQee+mjVr6pVXXtHSpUt18uRJrVixQn5+fnrllVdUrVo1s8Nze7NmLdNjjzXX44+3VNmyJTV06PMKCSmiBQu+MTs05AA/X29NGttRr0ct09lzztWgujVL6j//3a6fNh/Wn8fOaN7nW7R7X7xqVC1hUrS4XYM379aKvxJ0+PxFHUy6oHG/HlCIr48qFPB39OlZoZR+Ppmoj/Yd0e/nknX8Yoo2nkzUmctXTIwcOeWjj4eo/aPhKle+pCpVKq1RY17U8eOntHtXnNmhIQ9yiwRQulYFnDBhgtq1a6eIiAjNnTtXNWvW1IABA8wOza1dvnxFu3b9rvvuq+3U3rhxbW3btsekqJCTxgx7WKt/2K/1Gw9l2Ldp2xG1iKiokGIBkqRG94SpzN1FtO7HA7kdJnJY/nzXBmjOXbkq6dr96Q2KFdKfyRcVc08VLX3gHk1pVEONgwuZGCVc6XzSBUlSYKD/LXoiR1hsGRi3GAIOCgrS+fPnVbNmTYWHh+v5559XkyZNVKBAgVs+NyUlRSkpKU5tdvtl2e3ergrXrSQmnlNqapoKFy7o1F6kSEGdPHnGlJiQc9q1rq7qlUPV5slpme5/a8xyvTOinbZ+P0hXrqQqzTA0MHKZNm07msuRIqf1qRymX0+f1eHz15KAgt5e8svnqc5l7tLM/Uf10d4jurdoQY2sU0kDfv5NO06fMzli5CTDMBQzbq7q1K2o8hVKmh0O8iC3SADnzp2b5YTvetHR0RoxYoRTW2Tky4qK6ptT4d0RbNfdu2AYhtVuZ8hzQkMKaOQbbdT5hTlKuXw10z49n2qgujVKqttLn+nP42fUoO7dih7WVgknkzKtGOLO0K9qGZUN8FPfjTsdbR7/94GOTTitzw8fkyQdTEpW1aACalsqhAQwjxn99izt33dEn84bcevOyBkW+zfTLRLAhx9+2PH3P//8UzabTSVKZO0epiFDhmQYJrbbrVP9CAoqIE9PD506lejU/vffZ1WkSEFzgkKOqFGlhIoW9te3i3o72vLl81SDuqX1bOf6qthwtN7o96B69lug1T9c+xrFPftPqGqlEPXufh8J4B2qb5UwNSpWSP027tSpf8zsPXv5iq6mpenw/w0Lpjt6/oKqB2X/P89wX2NGzdKaNVs0Z26UQkIKmx0O8ii3SADT0tI0atQovffeezp//ryka4tCv/baaxo6dKg8PG58q6Ldbpfdbr+u1RrDv5Lk7e2lqlXL6ccft6l584aO9tjY7XrggfomRoZ/a/3Gg4po/6FT24RRj+r3uFOaPGO9PD085O2VT2nXzQBNTTXk4ab3nODmXqlSRveFFNKrG39T/EXnW1uuGob2nj2vkv6+Tu135ffViUvOfXFnMgxDY0bN0urvNmvWnOG66y5WcchNN0k18iS3SACHDh2qGTNmaOzYsWrcuLEMw9CPP/6oqKgoXbp0SaNHjzY7RLf27LPtNWjQeFWrVl61a1fSokXf6vjxk3ryydZmh4Z/IfnCZe37PcGp7cLFK0o8c8HRHrs5Tm+91lKXUq7oz2Nn1LBemDo+Uksj3mEG+J2mf9UyeiC0qIb9skcXrqYqyNtLkpR8NVWX064tA7Po0F8aXruifj19Ttv+Pqt7ixZUo2KF1P/nnTc7NO4Qo0bO1PKvf9QHkwYqf35fnfq/+7j9A/zk42OdwgZyh1skgHPmzNEnn3yiRx55xNFWs2ZNlShRQn369CEBvIU2be5XYuI5TZmyUAkJp1WhQmlNnx6pEiX432Ne9+LAxXqzf3NNGvu4Cgb66q9jZzTug+9YCPoO1K70te89n9igulP72B0HtOKvawn/hhOnNeG3g+pS9i71rRKmP5IvKnLrXv2WmJTr8SLnLVq4SpL0bLeRTu2jxvRW+0fDcz8gi7HaffM2wzBMX0HUx8dHv/76qypUqODUvm/fPtWqVUsXL168wTNvZH/OBQe3F1ptrtkhIBdVjGlldgjIRStb+ZkdAnKRl0ftW3dykTJT1rns2If6NHXZsW+XW4x416xZU5MmTcrQPmnSJNWoUcOEiAAAAPIutxgCjomJ0UMPPaTvvvtODRs2lM1mU2xsrP744w8tX77c7PAAAEAed/1yanmdW1QAmzZtqv379+vRRx/VmTNndPr0aXXo0EG7du3SrFmzzA4PAAAgT3GLCqAkhYaGZpjssWPHDs2ZM0czZ840KSoAAGAFFisAukcFEAAAALnHbSqAAAAAZqECCAAAgDzN1Apghw4dbrr/zJkzuRMIAACwNJvFSmKmJoCBgYG33P/MM8/kUjQAAMCqrDYEbGoCyBIvAAAAuY9JIAAAwPI8LFYBtNiINwAAAEgAAQCA5dlsrtuyY+rUqapRo4YKFCigAgUKqGHDhvrmm28c+w3DUFRUlEJDQ+Xr66vw8HDt2rUr26+XBBAAAMBN3HXXXRo7dqy2bNmiLVu2qFmzZmrXrp0jyYuJidH48eM1adIkbd68WSEhIWrevLmSkpKydR4SQAAAYHnuUgFs27at2rRpowoVKqhChQoaPXq0/P39tXHjRhmGoYkTJ2ro0KHq0KGDqlWrpjlz5ujChQuaP39+ts5DAggAAOBCKSkpOnfunNOWkpJyy+elpqZq4cKFSk5OVsOGDRUXF6f4+Hi1aNHC0cdut6tp06aKjY3NVkwkgAAAwPJsNpvLtujoaAUGBjpt0dHRN4xl586d8vf3l91uV+/evfXFF1+oSpUqio+PlyQFBwc79Q8ODnbsyyqWgQEAAJbnym8CGTJkiAYMGODUZrfbb9i/YsWK2r59u86cOaMlS5aoW7duWrdunWO/7bpxZcMwMrTdCgkgAACAC9nt9psmfNfz9vZWuXLlJEn16tXT5s2b9f7772vw4MGSpPj4eBUvXtzRPyEhIUNV8FYYAgYAAJbnLpNAMmMYhlJSUhQWFqaQkBCtWrXKse/y5ctat26dGjVqlK1jUgEEAABwE2+++aZat26tkiVLKikpSQsXLtTatWv17bffymazqX///hozZozKly+v8uXLa8yYMfLz81OXLl2ydR4SQAAAYHk5UanLCSdOnNDTTz+t48ePKzAwUDVq1NC3336r5s2bS5IGDRqkixcvqk+fPkpMTFT9+vW1cuVKBQQEZOs8NsMwDFe8AHPtNzsA5KLQanPNDgG5qGJMK7NDQC5a2crP7BCQi7w8apt27pqfrXfZsXc8db/Ljn27qAACAADLc5cKYG5hEggAAIDFUAEEAACW52GxCiAJIAAAsDyGgAEAAJCnUQEEAACWRwUQAAAAeRoVQAAAYHk2i80CoQIIAABgMVQAAQCA5VntHsAsJYBffvlllg/4yCOP3HYwAAAAcL0sJYDt27fP0sFsNptSU1P/TTwAAAC5jgpgJtLS0lwdBwAAgGmslgAyCQQAAMBibmsSSHJystatW6ejR4/q8uXLTvteeeWVHAkMAAAgt1hsFZjsJ4Dbtm1TmzZtdOHCBSUnJ6tQoUI6deqU/Pz8VKxYMRJAAAAAN5ftIeBXX31Vbdu21enTp+Xr66uNGzfqyJEjqlu3rt59911XxAgAAOBSNpvrNneU7QRw+/bteu211+Tp6SlPT0+lpKSoZMmSiomJ0ZtvvumKGAEAAJCDsp0Aenl5yfZ/6WxwcLCOHj0qSQoMDHT8HQAA4E5i83Dd5o6yfQ9g7dq1tWXLFlWoUEEREREaPny4Tp06pblz56p69equiBEAAAA5KNt56ZgxY1S8eHFJ0ttvv63ChQvrxRdfVEJCgqZPn57jAQIAALia1e4BzHYFsF69eo6/Fy1aVMuXL8/RgAAAAOBat7UOIAAAQF5ic9dSnYtkOwEMCwu76UU6dOjQvwoIAAAgt1ks/8t+Ati/f3+nx1euXNG2bdv07bff6vXXX8+puAAAAOAi2U4A+/Xrl2n75MmTtWXLln8dEAAAQG6zWgUwx1anad26tZYsWZJThwMAAICL5NgkkM8//1yFChXKqcMBAADkGqtVAG9rIeh/TgIxDEPx8fE6efKkpkyZkqPBAQAAIOdlOwFs166dUwLo4eGhokWLKjw8XJUqVcrR4ICsKDSwhdkhIBdt7D3J7BCQizyPDDM7BFiEBxXAm4uKinJBGAAAAMgt2Z4E4unpqYSEhAztf//9tzw9PXMkKAAAgNzkYXPd5o6yXQE0DCPT9pSUFHl7e//rgAAAAHKbhy3z/CavynIC+MEHH0i69lUpn3zyifz9/R37UlNT9cMPP3APIAAAwB0gywnghAkTJF2rAE6bNs1puNfb21t33323pk2blvMRAgAAuJi7DtW6SpYTwLi4OElSRESEli5dqqCgIJcFBQAAANfJ9j2Aa9ascUUcAAAApsmxr0a7Q2T79Xbs2FFjx47N0P7OO+/o8ccfz5GgAAAA4DrZTgDXrVunhx56KEN7q1at9MMPP+RIUAAAALnJw2a4bHNH2U4Az58/n+lyL15eXjp37lyOBAUAAADXyXYCWK1aNS1atChD+8KFC1WlSpUcCQoAACA3sRD0Lbz11lt67LHHdPDgQTVr1kyStHr1as2fP1+ff/55jgcIAADgalabBJLtBPCRRx7RsmXLNGbMGH3++efy9fVVzZo19f3336tAgQKuiBEAAAA5KNsJoCQ99NBDjokgZ86c0bx589S/f3/t2LFDqampORogAACAq7nrUK2r3HbF8/vvv9dTTz2l0NBQTZo0SW3atNGWLVtyMjYAAAC4QLYqgH/++admz56tmTNnKjk5WU888YSuXLmiJUuWMAEEAADcsWxuulyLq2S5AtimTRtVqVJFu3fv1ocffqhjx47pww8/dGVsAAAAcIEsVwBXrlypV155RS+++KLKly/vypgAAAByFfcA3sD69euVlJSkevXqqX79+po0aZJOnjzpytgAAADgAllOABs2bKiPP/5Yx48fV69evbRw4UKVKFFCaWlpWrVqlZKSklwZJwAAgMt4uHBzR9mOy8/PTz169NCGDRu0c+dOvfbaaxo7dqyKFSumRx55xBUxAgAAuBTfBZwNFStWVExMjP78808tWLAgp2ICAACAC93WQtDX8/T0VPv27dW+ffucOBwAAECuYhIIAAAA8rQcqQACAADcyaxWEbPa6wUAALA8KoAAAMDyuAcQAAAAeRoVQAAAYHnuul6fq5AAAgAAy2MIGAAAAHkaFUAAAGB5VquIWe31AgAAWJ7bVAAvXbqkX3/9VQkJCUpLS3Pa98gjj5gUFQAAsAImgZjg22+/1TPPPKNTp05l2Gez2ZSammpCVAAAAHmTWwwBv/zyy3r88cd1/PhxpaWlOW0kfwAAwNU8bK7b3JFbJIAJCQkaMGCAgoODzQ4FAAAgz3OLBLBjx45au3at2WEAAACLsloF0C3uAZw0aZIef/xxrV+/XtWrV5eXl5fT/ldeecWkyAAAgBW4RUUsF7lFAjh//nytWLFCvr6+Wrt2rWy2/58u22w2EkAAAIAc5BYJ4LBhwzRy5Ei98cYb8vCwWg4OAADMZrVlYNwi27p8+bI6depE8gcAAJAL3CLj6tatmxYtWmR2GAAAwKKYBGKC1NRUxcTEaMWKFapRo0aGSSDjx483KTIAAIC8xy0SwJ07d6p27dqSpN9++81p3z8nhAAAALiCWwyJ5iK3SADXrFljdggAAACW4RYJIAAAgJnc9V49V3GLBDAiIuKmQ73ff/99LkYDAACsxsYyMLmvVq1aqlmzpmOrUqWKLl++rK1bt6p69epmhwcAAJAroqOjdc899yggIEDFihVT+/bttW/fPqc+hmEoKipKoaGh8vX1VXh4uHbt2pWt87hFBXDChAmZtkdFRen8+fO5HA0AALAadxkCXrdunV566SXdc889unr1qoYOHaoWLVpo9+7dyp8/vyQpJiZG48eP1+zZs1WhQgWNGjVKzZs31759+xQQEJCl89gMw3Dbmufvv/+ue++9V6dPn87mM/e7JB64p2qzT5gdAnLRweFTzA4BuSj5yDCzQ0Au8rBVNe3cQ7asdtmxo+s9cNvPPXnypIoVK6Z169apSZMmMgxDoaGh6t+/vwYPHixJSklJUXBwsMaNG6devXpl6bhuUQG8kZ9++kk+Pj5mh3FHmDfva82YsVQnTyaqfPlSevPN51WvnnkfJPx7nSoWV6eKxRXqb5ck/X7mgqbtOKoNfyU6+vSpVUodK4SogHc+7TyVpFEbD+rgmQtmhYx/Yeirj2nYqx2d2uITziis3ovKl89TUa8/oZYRtRRWqpjOJV3U9xt26q2xC3X8ROINjog7zebNuzRzxn+1a9dBnTyZqA8nDdaDD9Y3OyzLcOU9cSkpKUpJSXFqs9vtstvtt3zu2bNnJUmFChWSJMXFxSk+Pl4tWrRwOlbTpk0VGxt7ZyWAHTp0cHpsGIaOHz+uLVu26K233jIpqjvH8uXrFR39iSIje6tOnSpauPBbPf98lL7+erJCQ4uZHR5uU3xyiib8EqejSZckSe3KFtOHzaqo41fbdPDMBfWodpeeqVJCwzbs1+FzF9WrZil93KKaHl76iy5cTTU5etyOXfv+0ENdRjsep6amSZL8fL1Vq1qYxn7whX7dfURBgfn1TuQz+s+Mgbrv4aFmhYscdvFiiipWuluPdmimfq/EmB0OclB0dLRGjBjh1BYZGamoqKibPs8wDA0YMED33XefqlWrJkmKj4+XJAUHBzv1DQ4O1pEjR7Ick1skgIGBgU6PPTw8VLFiRY0cOdIpw0XmZs1apscea67HH28pSRo69Hlt2LBVCxZ8o9de62ZydLhd6/50vvXhg21H1KlScdUsGqCDZy7o6SolNP3XP/Td0b8lSW+u36d1TzbQQ2WK6j/7480IGf/S1aupOnHybIb2c0kX9XDXMU5tA4bP1ob/jVbJ0ML649jfuRUiXKhJkzpq0qSO2WFYlocLZwEPGTJEAwYMcGrLSvXv5Zdf1q+//qoNGzZk2Hf96imGYWTryzPcIgGcNWuW2SHcsS5fvqJdu37XCy84Dx01blxb27btMSkq5DQPm9Ty7qLyzeep7QlJusvfR0X9vBV77P8P/11JM7Ql/qxqFStAAniHKhcWokObpygl5Yo2b/9dw2MW6fDRhEz7Fijgp7S0NJ05x5A/4O6yOtz7T3379tWXX36pH374QXfddZejPSQkRNK1SmDx4sUd7QkJCRmqgjfjFgngv5H5uPpl2e3eJkWUuxITzyk1NU2FCxd0ai9SpKBOnjxjSkzIOeUL+mneQ7Xk7emhC1dT1e/73Tp09oJqFb02y+vvi1ec+v998bJC/blv9k60edvveu7VqTpw6LiKFQ3UG30f1ZqlI1T3wdd1+ozzagh2u5fefqOzFi2LVdL5iyZFDOQt7jIL2DAM9e3bV1988YXWrl2rsLAwp/1hYWEKCQnRqlWrHF+je/nyZa1bt07jxo3L8nlMWwewUKFCOnXqlCQpKChIhQoVuuF2M9HR0QoMDHTaoqM/yo2X4FYyLwWbFAxyTNy5i3rsy63q+vV2Ld57XKPvr6gygX6O/YachyxstoxtuDOsXLtDy77ZpF37/tCaDb/p0e7X7gF7qmMTp3758nlq7qS+8rDZ1G/YTDNCBfIkD5vrtux46aWX9Nlnn2n+/PkKCAhQfHy84uPjdfHitf/s2Ww29e/fX2PGjNEXX3yh3377Td27d5efn5+6dOmS5fOYVgGcMGGCY62aiRMn3vZxMh9XP/pvQrujBAUVkKenh06dcp4J+PffZ1WkSEFzgkKOuZpm6I//mwSy6+/zqlrEX09VCdXMnX9Kkor4euvUP6qAhXy8M1QFcWe6cDFFu/b9obJhIY62fPk8NW9KP5UuWUytnxxF9Q/Ig6ZOnSpJCg8Pd2qfNWuWunfvLkkaNGiQLl68qD59+igxMVH169fXypUrs7wGoGRiAtitW7dM/55dmY+rW2P4V5K8vb1UtWo5/fjjNjVv3tDRHhu7XQ88wPIBeY1NNnl7eujP85d08sJlNQwN0t7TyZKkfB421QsJ1IQtcSZHiZzg7Z1PlcqF6sdNeyX9/+SvbFiIWnV6O8OwMIB/x9PsAP5PVpZnttlsioqKuuUs4psxLQE8d+5clvsWKFDAhZHc+Z59tr0GDRqvatXKq3btSlq06FsdP35STz7Z2uzQ8C/0q1Na6/9MVPyFFOXP56nWYUV1T0igeq/6TZI0d/dfer5GSR09d1FHzl3U8zVK6tLVVH196KTJkeN2RA/tqq+/26o/jp1SscIFNPiVRxXg76t5n/8gT08PzZ/WX7WrhanDszHy9PRQcNFrqyecPnNeV66w7E9ekJx8UUeP/v8JXH/+maA9e+IUGOiv0NCiJkaGvMi0BLBgwYJZnq6cmsovt5tp0+Z+JSae05QpC5WQcFoVKpTW9OmRKlGCNQDvZIV9vBXdpKKK+nor6fJV7U9MVu9Vv+mn42ckSTN/+1M++Tw0rEE5FbDn068nk/TCyt9YA/AOVaJ4IX06qa8KBwXo1Olz2rT1gJq2H66jf51SqbuKqG2LepKkTSucb/Ju8cRIrd/IjP+8YNdvB9Wt23DH43Fjr62Q0b59hKLH9jUrLMtw5TIw7si0r4Jbt26d4++HDx/WG2+8oe7du6thw2vDmD/99JPmzJmj6Ojo2xgi5qvgrISvgrMWvgrOWvgqOGsx86vgxmxf5bJjv1mrucuOfbtMqwA2bdrU8feRI0dq/Pjx6ty5s6PtkUceUfXq1TV9+vR/dY8gAADArbjLMjC5xbRlYP7pp59+Ur169TK016tXT5s2bTIhIgAAgLzLLRLAkiVLatq0aRnaP/roI5UsWdKEiAAAgJW4yzqAucUtvglkwoQJeuyxx7RixQo1aNBAkrRx40YdPHhQS5YsMTk6AACQ13m6aaLmKm5RAWzTpo3279+vRx55RKdPn9bff/+tdu3aaf/+/WrTpo3Z4QEAAOQpblEBlK4NA48ZM8bsMAAAgAW561Ctq7hFBVCS1q9fr6eeekqNGjXSX3/9JUmaO3euNmzYYHJkAAAAeYtbJIBLlixRy5Yt5evrq61btyolJUWSlJSURFUQAAC4nIfNcNnmjtwiARw1apSmTZumjz/+WF5eXo72Ro0aaevWrSZGBgAAkPe4xT2A+/btU5MmTTK0FyhQQGfOnMn9gAAAgKVwD6AJihcvrt9//z1D+4YNG1SmTBkTIgIAAMi73CIB7NWrl/r166eff/5ZNptNx44d07x58zRw4ED16dPH7PAAAEAe5+nCzR25xRDwoEGDdPbsWUVEROjSpUtq0qSJ7Ha7Bg4cqJdfftns8AAAAPIUt0gAJWn06NEaOnSodu/erbS0NFWpUkX+/v5mhwUAACzAavcAmpoA9ujRI0v9Zs6c6eJIAACAlbnrci2uYmoCOHv2bJUuXVq1a9eWYVjrwgMAAJjF1ASwd+/eWrhwoQ4dOqQePXroqaeeUqFChcwMCQAAWJCnxYaATZ0FPGXKFB0/flyDBw/WV199pZIlS+qJJ57QihUrqAgCAAC4iOnLwNjtdnXu3FmrVq3S7t27VbVqVfXp00elS5fW+fPnzQ4PAABYgIfNdZs7Mj0B/CebzSabzSbDMJSWlmZ2OAAAAHmS6QlgSkqKFixYoObNm6tixYrauXOnJk2apKNHj7IMDAAAyBVWqwCaOgmkT58+WrhwoUqVKqVnn31WCxcuVOHChc0MCQAAIM8zNQGcNm2aSpUqpbCwMK1bt07r1q3LtN/SpUtzOTIAAGAl7lqpcxVTE8BnnnlGNpvFrjgAAHA7niwEnXtmz55t5ukBAAAsyW2+CxgAAMAsps+KzWVWe70AAACWRwUQAABYntUmgVABBAAAsBgqgAAAwPKoAAIAACBPowIIAAAsj3UAAQAALIYhYAAAAORpVAABAIDlUQEEAABAnkYFEAAAWB4VQAAAAORpVAABAIDleVIBBAAAQF5GBRAAAFieBwtBAwAAWIvVhkSt9noBAAAsjwogAACwPJaBAQAAQJ5GBRAAAFgey8AAAAAgT6MCCAAALM9qy8BQAQQAALAYKoAAAMDyrDYLmAQQAABYntUSQIaAAQAALIYKIO54v3YrZHYIyEXnujxtdgjIRfEXD5kdAnJRqF9V085ttYqY1V4vAACA5VEBBAAAlmfjHkAAAADkZVQAAQCA5VmsAEgFEAAAwGqoAAIAAMuz2j2AJIAAAMDyrDYkarXXCwAAYHlUAAEAgOXZbIbZIeQqKoAAAAAWQwUQAABYnsXmgFABBAAAsBoqgAAAwPKstgwMFUAAAACLoQIIAAAsz2IFQBJAAAAAD4tlgAwBAwAAWAwVQAAAYHkWKwBSAQQAALAaKoAAAMDyWAYGAAAAeRoVQAAAYHkWKwBSAQQAAHAnP/zwg9q2bavQ0FDZbDYtW7bMab9hGIqKilJoaKh8fX0VHh6uXbt2ZescJIAAAMDybC7csis5OVk1a9bUpEmTMt0fExOj8ePHa9KkSdq8ebNCQkLUvHlzJSUlZfkcDAEDAADLc6eFoFu3bq3WrVtnus8wDE2cOFFDhw5Vhw4dJElz5sxRcHCw5s+fr169emXpHFQAAQAAXCglJUXnzp1z2lJSUm7rWHFxcYqPj1eLFi0cbXa7XU2bNlVsbGyWj0MCCAAALM+VQ8DR0dEKDAx02qKjo28rzvj4eElScHCwU3twcLBjX1YwBAwAAOBCQ4YM0YABA5za7Hb7vzqm7bqFCw3DyNB2MySAAADA8mw2w2XHttvt/zrhSxcSEiLpWiWwePHijvaEhIQMVcGbYQgYAADgDhEWFqaQkBCtWrXK0Xb58mWtW7dOjRo1yvJxqAACAADLc6NJwDp//rx+//13x+O4uDht375dhQoVUqlSpdS/f3+NGTNG5cuXV/ny5TVmzBj5+fmpS5cuWT4HCSAAAIAb2bJliyIiIhyP0+8f7Natm2bPnq1Bgwbp4sWL6tOnjxITE1W/fn2tXLlSAQEBWT6HzTAM1w16m2a/2QEgF6UZV8wOAbno3JUjZoeAXHThaqrZISAXhfq1Ne3ch5K+ctmxywSY97puhHsAAQAALIYhYAAAYHlWq4iRAAIAAMvLxhJ6eYLVEl4AAADLowIIAAAsz2IFQCqAAAAAVkMFEAAAWB73AAIAACBPowIIAAAsz2IFQCqAAAAAVkMFEAAAWJ6HxUqAJIAAAMDyLJb/MQQMAABgNW5RAUxNTdXs2bO1evVqJSQkKC0tzWn/999/b1JkAADACmw2w+wQcpVbJID9+vXT7Nmz9dBDD6latWqyWW0xHgAAgFzkFgngwoULtXjxYrVp08bsUAAAgAVZrfTkFvcAent7q1y5cmaHAQAAYAlukQC+9tprev/992UY1hp/BwAA7sFmc93mjkwbAu7QoYPT4++//17ffPONqlatKi8vL6d9S5cuzc3QAAAA8jTTEsDAwECnx48++qhJkeQN8+Z9rRkzlurkyUSVL19Kb775vOrVq2p2WMhhmzfv0swZ/9WuXQd18mSiPpw0WA8+WN/ssOBCCSfOaPKE/yl2wx6lpFxRqdJFNXTEk6pctaTZoSGHpV5N1eyPVuq75Vt1+u8kFS5SQC3b1tPTzz8oDw+3GLDL09y0UOcypiWAs2bNMuvUec7y5esVHf2JIiN7q06dKlq48Fs9/3yUvv56skJDi5kdHnLQxYspqljpbj3aoZn6vRJjdjhwsXNnL+iFZz5QnXvKa+LUFxRUKEB//XFKAQV8zQ4NLrBg9hp9+flPemPkkworG6J9u/7QuKjFyh/gq45d7jc7vDzPaim2W8wCxr8za9YyPfZYcz3+eEtJ0tChz2vDhq1asOAbvfZaN5OjQ05q0qSOmjSpY3YYyCVzZ65WsZCCGj6qs6MttEQhEyOCK+369YgaN62mhvdXkSSFhBbS6m+3a//uP0yODHmRWySAtWvXznTtP5vNJh8fH5UrV07du3dXRESECdG5t8uXr2jXrt/1wgsdndobN66tbdv2mBQVgJzww9pdatCoooYMmK1tvxxU0WKBeqxTY7Xv2NDs0OAC1WuF6cvPf9IfR06qZOmi+n3fMf22PU4vDWxndmiW4K6TNVzFLRLAVq1aaerUqapevbruvfdeGYahLVu26Ndff1X37t21e/duPfjgg1q6dKnatXP+IKSkpCglJcWpzW6/LLvdOzdfgmkSE88pNTVNhQsXdGovUqSgTp48Y0pMAHLGsT//1tLFser8TLi6P/+gdu08qvFjv5C3dz61eeQes8NDDuv8bISSz19St0dj5OFpU1qqoZ4vtdIDrWubHRryILdIAE+dOqXXXntNb731llP7qFGjdOTIEa1cuVKRkZF6++23MySA0dHRGjFihFNbZOTLiorq6/K43cn1FVTDMCz3vxkgr0lLM1S5akn16feQJKli5bsUdzBeSxb9SAKYB61ZsV2rlv+iYWO66O6yIfp93zFNfve/Kly0gFrxfucCa/2j6Rb3PC5evFidO3fO0P7kk09q8eLFkqTOnTtr3759GfoMGTJEZ8+eddqGDOnl8pjdRVBQAXl6eujUqUSn9r//PqsiRQqaExSAHFGkaAGFlQ12aru7TLBOxJ8xJyC41LSJ/1PnZ5upWavaKlO+uFo8XFcduzbR/Fnfmx0a8iC3SAB9fHwUGxuboT02NlY+Pj6SpLS0NNnt9gx97Ha7ChQo4LRZZfhXkry9vVS1ajn9+OM2p/bY2O2qXbuySVEByAk1aoXpyOEEp7ajhxMUUjzIpIjgSimXrsjjuqEbDw+bjDS+JCE32Fz4xx25xRBw37591bt3b/3yyy+65557ZLPZtGnTJn3yySd68803JUkrVqxQ7drcB5GZZ59tr0GDxqtatfKqXbuSFi36VsePn9STT7Y2OzTksOTkizp6NN7x+M8/E7RnT5wCA/0VGlrUxMjgCp2faarnnn5fsz9epQda1tLunUe1bMlGDRn+hNmhwQUaNqmiz2asVrHiBRVWNkQH9v6l/3z2g1q3Z/gXOc9muMn3r82bN0+TJk1yDPNWrFhRffv2VZcuXSRJFy9edMwKvrX9LozUPaUvBJ2QcFoVKpTWkCHP6Z57qpkdVq5IM66YHUKu2fTzb+rWbXiG9vbtIxQ91hr3vZ67csTsEHLVhnW7NGXi1/rj6EmFliikzs+EW2oW8IWrqWaHkGsuJF/SzCkrtOH7nUpMPK8iRQPVrFUtPfNCc3l5uUW9xuVC/dqadu4zl5e77NgFvdu47Ni3y20SwJxlvQTQyqyUAMJ6CaDVWSkBhNkJ4DcuO3ZBb/cbkXOLewABAACQe0yrKRcqVEj79+9XkSJFFBQUlOlC0OlOnz6di5EBAACrcdfJGq5iWgI4YcIEBQQEOP5+swQQAAAAOcfUewDPnTuXpX4FChTI5pG5B9BKuAfQWrgH0Fq4B9BazLwH8OzlFS47dqB3S5cd+3aZOq2oYMGCWar8pabyCwAAACCnmJoArlmzxvF3wzDUpk0bffLJJypRooSJUQEAAKux2aw1L9bUBLBp06ZOjz09PdWgQQOVKVPGpIgAAADyPmusLAkAAHBT1pqMSgIIAAAsz2rLwLjdgDfLwQAAALiWqRXADh06OD2+dOmSevfurfz58zu1L126NDfDAgAAFmO1CqCpCWBgYKDT46eeesqkSAAAAKzD1ARw1qxZZp4eAADg/7jdXXEuZa1XCwAAAGYBAwAAWG0SKhVAAAAAi6ECCAAAwCxgAAAAa7HaMjAMAQMAAFgMFUAAAACL1cSs9WoBAABABRAAAIB7AAEAAJCnUQEEAACWx0LQAAAAyNOoAAIAAFjsHkASQAAAYHk2iw2KWuvVAgAAgAogAACA1YaAqQACAABYDBVAAABgeSwDAwAAgDyNCiAAAAD3AAIAACAvowIIAAAsz2rrAJIAAgAAMAQMAACAvIwKIAAAsDwbFUAAAADkZVQAAQCA5bEQNAAAAPI0KoAAAAAWq4lZ69UCAACACiAAAACzgAEAAJCnUQEEAACwWAWQBBAAAFgey8AAAAAgTyMBBAAAkIcLt+ybMmWKwsLC5OPjo7p162r9+vW3/coyQwIIAADgRhYtWqT+/ftr6NCh2rZtm+6//361bt1aR48ezbFz2AzDMHLsaG5jv9kBIBelGVfMDgG56NyVI2aHgFx04Wqq2SEgF4X6tTXx7K7MHSpkq3f9+vVVp04dTZ061dFWuXJltW/fXtHR0TkSERVAAAAAF0pJSdG5c+ectpSUlEz7Xr58Wb/88otatGjh1N6iRQvFxsbmWEx5dBZw9jLtvCAlJUXR0dEaMmSI7Ha72eHkKg9rTdySZO33u6B3VbNDyHXWfr/NjiD3Wfn9Npfrcofo6CiNGDHCqS0yMlJRUVEZ+p46dUqpqakKDg52ag8ODlZ8fHyOxZRHh4Ct59y5cwoMDNTZs2dVoEABs8OBi/F+Wwvvt7Xwfuc9KSkpGSp+drs90wT/2LFjKlGihGJjY9WwYUNH++jRozV37lzt3bs3R2LKoxVAAAAA93CjZC8zRYoUkaenZ4ZqX0JCQoaq4L/BPYAAAABuwtvbW3Xr1tWqVauc2letWqVGjRrl2HmoAAIAALiRAQMG6Omnn1a9evXUsGFDTZ8+XUePHlXv3r1z7BwkgHmE3W5XZGQkNwxbBO+3tfB+WwvvNzp16qS///5bI0eO1PHjx1WtWjUtX75cpUuXzrFzMAkEAADAYrgHEAAAwGJIAAEAACyGBBAAAMBiSAABwI2tXbtWNptNZ86ckSTNnj1bBQsWNDUm5KzbeU+7d++u9u3buyQeWAMJoBvp3r27bDabxo4d69S+bNky2WwW/L4zi+EX+p0p/XOb2fIMffr0kc1mU/fu3XPsfJ06ddL+/a780nrkpBt9rv+Z2POewgwkgG7Gx8dH48aNU2JiotmhAMiikiVLauHChbp48aKj7dKlS1qwYIFKlSqVo+fy9fVVsWLFcvSYMBfvKcxAAuhmHnzwQYWEhCg6OvqGfZYsWaKqVavKbrfr7rvv1nvvvee0/+6779aYMWPUo0cPBQQEqFSpUpo+fbpTn7/++kudOnVSUFCQChcurHbt2unw4cOueEnIAevWrdO9994ru92u4sWL64033tDVq1clSV999ZUKFiyotLQ0SdL27dtls9n0+uuvO57fq1cvde7c2ZTYraBOnToqVaqUli5d6mhbunSpSpYsqdq1azvaDMNQTEyMypQpI19fX9WsWVOff/6507GWL1+uChUqyNfXVxERERk+l9cPF2ZWYerfv7/Cw8Mdj8PDw9W3b1/1799fQUFBCg4O1vTp05WcnKxnn31WAQEBKlu2rL755pt/fS2QfZkNAY8aNUrFihVTQECAnnvuOb3xxhuqVatWhue+++67Kl68uAoXLqyXXnpJV65cyZ2gcccjAXQznp6eGjNmjD788EP9+eefGfb/8ssveuKJJ/Tkk09q586dioqK0ltvvaXZs2c79XvvvfdUr149bdu2TX369NGLL77o+ALpCxcuKCIiQv7+/vrhhx+0YcMG+fv7q1WrVrp8+XJuvExkw19//aU2bdronnvu0Y4dOzR16lTNmDFDo0aNkiQ1adJESUlJ2rZtm6RryWKRIkW0bt06xzHWrl2rpk2bmhK/VTz77LOaNWuW4/HMmTPVo0cPpz7Dhg3TrFmzNHXqVO3atUuvvvqqnnrqKcd79ccff6hDhw5q06aNtm/f7viHPyfMmTNHRYoU0aZNm9S3b1+9+OKLevzxx9WoUSNt3bpVLVu21NNPP60LFy7kyPlw++bNm6fRo0dr3Lhx+uWXX1SqVClNnTo1Q781a9bo4MGDWrNmjebMmaPZs2dn+LcAuCEDbqNbt25Gu3btDMMwjAYNGhg9evQwDMMwvvjiCyP9rerSpYvRvHlzp+e9/vrrRpUqVRyPS5cubTz11FOOx2lpaUaxYsWMqVOnGoZhGDNmzDAqVqxopKWlOfqkpKQYvr6+xooVK1zy2nBr/3z//+nNN9/M8H5NnjzZ8Pf3N1JTUw3DMIw6deoY7777rmEYhtG+fXtj9OjRhre3t3Hu3Dnj+PHjhiRjz549ufI6rCb9fTt58qRht9uNuLg44/Dhw4aPj49x8uRJo127dka3bt2M8+fPGz4+PkZsbKzT83v27Gl07tzZMAzDGDJkiFG5cmWn93rw4MGGJCMxMdEwDMOYNWuWERgYmOH8/9SvXz+jadOmjsdNmzY17rvvPsfjq1evGvnz5zeefvppR1v6z8lPP/30L68I/qlbt26Gp6enkT9/fqfNx8fH8b5e/57Wr1/feOmll5yO07hxY6NmzZpOxy1durRx9epVR9vjjz9udOrUydUvCXkEFUA3NW7cOM2ZM0e7d+92at+zZ48aN27s1Na4cWMdOHBAqampjrYaNWo4/m6z2RQSEqKEhARJ16qIv//+uwICAuTv7y9/f38VKlRIly5d0sGDB134qnA79uzZo4YNGzpNBGrcuLHOnz/vqBKHh4dr7dq1MgxD69evV7t27VStWjVt2LBBa9asUXBwsCpVqmTWS7CEIkWK6KGHHtKcOXM0a9YsPfTQQypSpIhj/+7du3Xp0iU1b97c8bnz9/fXp59+6vjc7dmzRw0aNHB6rxs2bJgj8f3zd4Knp6cKFy6s6tWrO9qCg4MlyfF7AjknIiJC27dvd9o++eSTG/bft2+f7r33Xqe26x9LUtWqVeXp6el4XLx4cd4/ZBnfBeymmjRpopYtW+rNN990mkFoGEaGGcFGJt/m5+Xl5fTYZrM57hFLS0tT3bp1NW/evAzPK1q0aA5Ej5x0s/c8vT08PFwzZszQjh075OHhoSpVqqhp06Zat26dEhMTGf7NJT169NDLL78sSZo8ebLTvvTP39dff60SJUo47Uv/ztfMPsu34uHhkeF5md0HltnvhH+2pf8spceJnJM/f36VK1fOqS2zW3z+6d/+ngduhQTQjY0dO1a1atVShQoVHG1VqlTRhg0bnPrFxsaqQoUKTv8TvJk6depo0aJFKlasmAoUKJCjMSPnValSRUuWLHFKBGNjYxUQEOBIJNLvA5w4caKaNm0qm82mpk2bKjo6WomJierXr5+ZL8Ey/nkfbcuWLZ32ValSRXa7XUePHr1hQl6lShUtW7bMqW3jxo03PWfRokX122+/ObVt3749Q3KAO0fFihW1adMmPf300462LVu2mBgR8iKGgN1Y9erV1bVrV3344YeOttdee02rV6/W22+/rf3792vOnDmaNGmSBg4cmOXjdu3aVUWKFFG7du20fv16xcXFad26derXr98t/1cK1zp79myGoaIXXnhBf/zxh/r27au9e/fqv//9ryIjIzVgwAB5eFz7CAcGBqpWrVr67LPPHLM/mzRpoq1bt2r//v1OM0LhOp6entqzZ4/27NmT4T9kAQEBGjhwoF599VXNmTNHBw8e1LZt2zR58mTNmTNHktS7d28dPHhQAwYM0L59+zR//vxb3tTfrFkzbdmyRZ9++qkOHDigyMjIDAkh7ix9+/bVjBkzNGfOHB04cECjRo3Sr7/+ynqwyFEkgG7u7bffdir916lTR4sXL9bChQtVrVo1DR8+XCNHjszWQrN+fn764YcfVKpUKXXo0EGVK1dWjx49dPHiRSqCJlu7dq1q167ttEVGRmr58uXatGmTatasqd69e6tnz54aNmyY03MjIiKUmprqSPaCgoJUpUoVFS1aVJUrVzbh1VhTgQIFbvg5evvttzV8+HBFR0ercuXKatmypb766iuFhYVJkkqVKqUlS5boq6++Us2aNTVt2jSNGTPmpudr2bKl3nrrLQ0aNEj33HOPkpKS9Mwzz+T460Lu6dq1q4YMGaKBAweqTp06iouLU/fu3eXj42N2aMhDbMbt3HQCAAByTfPmzRUSEqK5c+eaHQryCO4BBADAjVy4cEHTpk1Ty5Yt5enpqQULFui7777TqlWrzA4NeQgVQAAA3MjFixfVtm1bbd26VSkpKapYsaKGDRumDh06mB0a8hASQAAAAIthEggAAIDFkAACAABYDAkgAACAxZAAAgAAWAwJIAAAgMWQAAJwW1FRUapVq5bjcffu3dW+fftcj+Pw4cOy2Wzavn17rp8bAFyBBBBAtnXv3l02m002m01eXl4qU6aMBg4cqOTkZJee9/3337/ld+OmI2kDgBvjm0AA3JZWrVpp1qxZunLlitavX6/nnntOycnJmjp1qlO/K1euyMvLK0fOGRgYmCPHAQCrowII4LbY7XaFhISoZMmS6tKli7p27aply5Y5hm1nzpypMmXKyG63yzAMnT17Vi+88IKKFSumAgUKqFmzZtqxY4fTMceOHavg4GAFBASoZ8+eunTpktP+64eA09LSNG7cOJUrV052u12lSpXS6NGjJUlhYWGSpNq1a8tmsyk8PNzxvFmzZqly5cry8fFRpUqVNGXKFKfzbNq0SbVr15aPj4/q1aunbdu25eCVAwDzUQEEkCN8fX115coVSdLvv/+uxYsXa8mSJfL09JQkPfTQQypUqJCWL1+uwMBAffTRR3rggQe0f/9+FSpUSIsXL1ZkZKQmT56s+++/X3PnztUHH3ygMmXK3PCcQ4YM0ccff6wJEybovvvu0/Hjx7V3715J15K4e++9V999952qVq0qb29vSdLHH3+syMhITZo0SbVr19a2bdv0/PPPK3/+/OrWrZuSk5P18MMPq1mzZvrss88UFxenfv36ufjqAUAuMwAgm7p162a0a9fO8fjnn382ChcubDzxxBNGZGSk4eXlZSQkJDj2r1692ihQoIBx6dIlp+OULVvW+OijjwzDMIyGDRsavXv3dtpfv359o2bNmpme99y5c4bdbjc+/vjjTGOMi4szJBnbtm1zai9ZsqQxf/58p7a3337baNiwoWEYhvHRRx8ZhQoVMpKTkx37p06dmumxAOBOxRAwgNvyv//9T/7+/vLx8VHDhg3VpEkTffjhh5Kk0qVLq2jRoo6+v/zyi86fP6/ChQvL39/fscXFxengwYOSpD179qhhw4ZO57j+8T/t2bNHKSkpeuCBB7Ic88mTJ/XHH3+oZ8+eTnGMGjXKKY6aNWvKz88vS3EAwJ2IIWAAtyUiIkJTp06Vl5eXQkNDnSZ65M+f36lvWlqaihcvrrVr12Y4TsGCBW/r/L6+vtl+TlpamqRrw8D169d32pc+VG0Yxm3FAwB3EhJAALclf/78KleuXJb61qlTR/Hx8cqXL5/uvvvuTPtUrlxZGzdu1DPPPONo27hx4w2PWb58efn6+mr16tV67rnnMuxPv+cvNTXV0RYcHKwSJUro0KFD6tq1a6bHrVKliubOnauLFy86ksybxQEAdyKGgAG43IMPPqiGDRuqffv2WrFihQ4fPqzY2FgNGzZMW7ZskST169dPM2fO1MyZM7V//35FRkZq165dNzymj4+PBg8erEGDBunTTz/VwYMHtXHjRs2YMUOSVKxYMfn6+urbb7/ViRMndPbsWUnXFpeOjo7W+++/r/3792vnzp2aNWuWxo8fL0nq0qWLPDw81LNnT+3evVvLly/Xu+++6+IrBAC5iwQQgMvZbDYtX75cTZo0UY8ePVShQgU9+eSTOnz4sIKDgyVJnTp10vDhwzV48GDVrVtXR44c0YsvvnjT47711lt67bXXNHz4cFWuXFmdOnVSQkKCJClfvnz64IMP9NFHHyk0NFTt2rWTJD333HP65JNPNHv2bFWvXl1NmzbV7NmzHcvG+Pv766uvvtLu3btVu3ZtDR06VOPGjXPh1QGA3GczuOEFAADAUqgAAgAAWAwJIAAAgMWQAAIAAFgMCSAAAIDFkAACAABYDAkgAACAxZAAAgAAWAwJIAAAgMWQAAIAAFgMCSAAAIDFkAACAABYzP8DLBpfOkHv0AkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#parte 4 el modelo\n",
    "# Train the Classifier\n",
    "# We use a simple Logistic Regression model.\n",
    "bert_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "bert_classifier.fit(X_train_bert, y_train_level)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred_bert = bert_classifier.predict(X_test_bert)\n",
    "\n",
    "\n",
    "# Define the order for the report and confusion matrix\n",
    "category_order = ['None', 'Low', 'Medium', 'High']\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n BERT + Logistic Regression Evaluation (Polarization Level) \\n\")\n",
    "print(classification_report(y_test_level, y_pred_bert, labels=category_order))\n",
    "\n",
    "# Visualize the Confusion Matrix\n",
    "cm_bert = confusion_matrix(y_test_level, y_pred_bert, labels=category_order)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_bert, annot=True, fmt='d',\n",
    "            xticklabels=category_order, yticklabels=category_order,\n",
    "            cmap='YlGnBu')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('BERT + Logistic Regression Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab93072-6c21-45bb-b216-5e181808ee8a",
   "metadata": {},
   "source": [
    "**MODEL 6.: BERT + lightGBM APPLICATION** \n",
    "\n",
    "We are going to proceed after vectorization happens with BERT to use the framework fo LGBM to conducnt the classification of the sanitised text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b1a9ec-ad3f-4234-88c9-cca2359180a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Complete: Data is prepared.\n"
     ]
    }
   ],
   "source": [
    "#Step 1: prepare the data\n",
    "\n",
    "# Load the small, human-labeled dataset (in this case and for this project we use only GPT coded training dataset)\n",
    "labeled_filepath = \"data/sample/debate_tweets_sample_1000_for_annotation_labeled.csv\"\n",
    "df_labeled = pd.read_csv(labeled_filepath)\n",
    "\n",
    "# Clean and prepare the data\n",
    "df_multi = df_labeled[df_labeled['polar_score'] != 'INV'].copy()\n",
    "df_multi['polar_score'] = pd.to_numeric(df_multi['polar_score'])\n",
    "\n",
    "# Define the function to create the 4 polarization levels\n",
    "# we opt for this path to avoid the problem of not enough data classified in the polar scores.\n",
    "def get_polarization_level(score):\n",
    "    score = int(score)\n",
    "    if score in [-7, -6, 6, 7]: return 'High'\n",
    "    elif score in [-5, -4, 4, 5]: return 'Medium'\n",
    "    elif score in [-3, -2, 2, 3]: return 'Low'\n",
    "    elif score in [-1, 0, 1]: return 'None'\n",
    "    return None\n",
    "\n",
    "# Create the new target column\n",
    "df_multi['polarization_level'] = df_multi['polar_score'].apply(get_polarization_level)\n",
    "\n",
    "# Create the training and testing splits\n",
    "X_train_level, X_test_level, y_train_level, y_test_level = train_test_split(\n",
    "    df_multi['santext'].fillna(''),\n",
    "    df_multi['polarization_level'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_multi['polarization_level']\n",
    ")\n",
    "\n",
    "print(\"Step 1 Complete: Data is prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c05cd50-890f-405a-a2d5-6efc335f259f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DistilBertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# FINAL MODEL TRAINING AND SAVING\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Step 2: Using BERT features\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# You may need to run: pip install transformers torch if it is not installed.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the pre-trained DistilBERT tokenizer and model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m DistilBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m DistilBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Set the device to GPU if available, otherwise CPU\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DistilBertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# FINAL MODEL TRAINING AND SAVING\n",
    "#Step 2: Using BERT features\n",
    "# You may need to run: pip install transformers torch if it is not installed.\n",
    "\n",
    "\n",
    "# Load the pre-trained DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.extend(cls_embeddings)\n",
    "    print(\"Embedding generation complete.\")\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Generate embeddings. This creates the 'X_train_bert' and 'X_test_bert' variables.\n",
    "X_train_bert = get_bert_embeddings(X_train_level.tolist())\n",
    "X_test_bert = get_bert_embeddings(X_test_level.tolist())\n",
    "\n",
    "print(\"\\nStep 2 Complete: BERT features have been created.\")\n",
    "print(f\"Shape of training features: {X_train_bert.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d41ef5-7909-4233-a6dd-b1270e59101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL MODEL TRAINING AND SAVING\n",
    "\n",
    "print(\"Training the champion model (BERT + LightGBM) on the designated training set\")\n",
    "\n",
    "# Initialize a new instance of the champion classifier\n",
    "champion_classifier = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Train (fit) the classifier ONLY on the training data embeddings and labels\n",
    "# X_train_bert was created in the previous step\n",
    "champion_classifier.fit(X_train_bert, y_train_level)\n",
    "\n",
    "# filename\n",
    "CHAMPION_MODEL_FILENAME = 'level_classifier_champion_final.pkl'\n",
    "\n",
    "# Save the trained classifier to a file using joblib\n",
    "joblib.dump(champion_classifier, CHAMPION_MODEL_FILENAME)\n",
    "\n",
    "print(f\"\\nChampion model trained on {len(X_train_bert)} samples.\")\n",
    "print(f\"Model saved successfully as '{CHAMPION_MODEL_FILENAME}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2eabe65-29bc-4761-87dd-575630fceb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In theory, this will be the line to train the code on the fukll dataset,\n",
    "#HIOWEVER, try this lines only i your computer run with a large quantity of memory\n",
    "# Define category order for reports\n",
    "#category_order = ['None', 'Low', 'Medium', 'High']\n",
    "\n",
    "# Train the Classifier\n",
    "#print(\"\\nStep 3: Training LightGBM classifier on BERT features \")\n",
    "#lgbm_on_bert_classifier = LGBMClassifier(random_state=42)\n",
    "#lgbm_on_bert_classifier.fit(X_train_bert, y_train_level)\n",
    "\n",
    "# Make Predictions\n",
    "#y_pred_lgbm_bert = lgbm_on_bert_classifier.predict(X_test_bert)\n",
    "\n",
    "# Evaluation\n",
    "#print(\"\\n BERT Features + LightGBM Evaluation (Polarization Level) \\n\")\n",
    "#print(classification_report(y_test_level, y_pred_lgbm_bert, labels=category_order))\n",
    "\n",
    "# Visualize the Confusion Matrix\n",
    "#cm_lgbm_bert = confusion_matrix(y_test_level, y_pred_lgbm_bert, labels=category_order)\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#sns.heatmap(cm_lgbm_bert, annot=True, fmt='d',\n",
    "         #   xticklabels=category_order, yticklabels=category_order,\n",
    "         #   cmap='plasma')\n",
    "#plt.xlabel('Predicted')\n",
    "#plt.ylabel('Actual')\n",
    "#plt.title('BERT Features + LightGBM Confusion Matrix')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb9066-ff4d-4669-ae5b-49fad15a8e6e",
   "metadata": {},
   "source": [
    "**FINAL MODEL SAVE BERT * LIGHTBMC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7762ddd9-b74e-4547-9eea-8e561845b12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up models \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# BERT Model & Tokenizer Setup \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting up models \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m DistilBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m DistilBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# PART 8: Production the best Model (BERT + LightGBM). \n",
    "# This was the model with a better F after comapring training and validation\n",
    "# Note: this script was revised, due to the constrictions of the IOS and memory system of the original computer, \n",
    "# we need to divide the batch fo analysys due to this constrcitions. \n",
    "# If you have a better machine you can run the complete code without dividing it\n",
    "\n",
    "\n",
    "# Configuration\n",
    "FULL_DATASET_PATH = 'debate_tweets_classified.csv'\n",
    "CHAMPION_MODEL_FILENAME = 'level_classifier_champion_final.pkl'\n",
    "FINAL_OUTPUT_FILENAME = 'debate_tweets_classified_COMPLETE.csv'\n",
    "NUM_CHUNKS = 16 # Switched to 16 chunks\n",
    "BERT_BATCH_SIZE = 32\n",
    "TEMP_CHUNK_DIR = 'temp_chunks'\n",
    "\n",
    "# BERT Model & Tokenizer Setup \n",
    "print(\"Setting up models \")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "bert_model.to(device)\n",
    "loaded_champion_classifier = joblib.load(CHAMPION_MODEL_FILENAME) # this was the model who champions all others to maange this task\n",
    "print(f\" Models loaded successfully on device: {device}\")\n",
    "\n",
    "# Embedding Function\n",
    "def get_bert_embeddings(texts, batch_size=BERT_BATCH_SIZE):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"✨ Generating Embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.extend(cls_embeddings)\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Create Temp Directory & Calculate Chunk Size\n",
    "os.makedirs(TEMP_CHUNK_DIR, exist_ok=True)\n",
    "total_rows = sum(1 for row in open(FULL_DATASET_PATH, 'r', encoding='utf-8')) - 1\n",
    "chunk_size = int(np.ceil(total_rows / NUM_CHUNKS))\n",
    "# Read header to pass to chunks\n",
    "header = pd.read_csv(FULL_DATASET_PATH, nrows=0).columns.tolist()\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Number of chunks: {NUM_CHUNKS}\")\n",
    "print(f\"Rows per chunk: {chunk_size}\")\n",
    "print(\"\\n SETUP COMPLETE. You can now proceed to process the chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae541c1-7001-4814-aa70-c611db87a1e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# and the next code wil run the batch in each cell. So we need to copy and paste this model 16 times.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the index for the chunk you want to process (0 for the first one)\n",
    "chunk_index = 0\n",
    "\n",
    "# and the next code wil run the batch in each cell. So we need to copy and paste this model 16 times.\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\" Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\" Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76ebb16-2919-403a-b858-3b107ed8e618",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 1\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\" Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"✅ Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f63de0-c571-44f2-9e59-458d02cd4ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 2\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d1f2ce-972e-42bc-bb43-5046d88381a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 3\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc475ae4-bc39-40a2-b9d3-4dfcb465f93c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 4\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5718d356-4638-48ab-a2c9-a66735e9cf34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 5\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2759089-145d-4d72-ad09-ea7719bdab8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 6\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\"Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\" Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\" Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa7e560c-70e8-4518-8adb-b69fd4e786ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 7\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff87ff83-1add-47da-a8e7-7fc872b30e3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 8\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b34337a1-b9e1-4727-8646-524ad3292dec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 9\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\"Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c02c38-56b7-43a9-9b8a-b368588797a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 10\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\"Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"✅ Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b0bda12-707d-4fca-8a61-0e8cf41af24c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 11\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\" Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\" Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2941f242-24d7-46a5-974e-0486a7b86027",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 12\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\"Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS}\")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4e0d382-7f99-4485-8bb1-71acaa9f132d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m13\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 13\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\"Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\" Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\" Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e15bd2f-7bbc-4934-bfcb-16a05a24f9b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m\n\u001b[0;32m----> 3\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 14\n",
    "\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"✅ Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\"Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f70035f3-4ac5-4be4-aa85-2f2e13e52d74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m chunk_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Do not change the code below ---\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m chunk_output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Preparing to process Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chunk_output_path):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "chunk_index = 15\n",
    "\n",
    "# Do not change the code below ---\n",
    "chunk_output_path = os.path.join(TEMP_CHUNK_DIR, f'chunk_{chunk_index}_complete.csv')\n",
    "print(f\" Preparing to process Chunk {chunk_index + 1} of {NUM_CHUNKS} \")\n",
    "\n",
    "if os.path.exists(chunk_output_path):\n",
    "    print(f\"Chunk {chunk_index + 1} is already complete. Skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        # Calculate how many rows to skip to get to the start of our chunk\n",
    "        rows_to_skip = 1 + chunk_index * chunk_size # +1 for header\n",
    "        \n",
    "        # Read only the specific chunk from the large CSV\n",
    "        chunk_df = pd.read_csv(\n",
    "            FULL_DATASET_PATH,\n",
    "            header=None,\n",
    "            names=header,\n",
    "            skiprows=rows_to_skip,\n",
    "            nrows=chunk_size,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_texts = chunk_df['santext'].fillna('').tolist()\n",
    "        if chunk_texts: # Ensure there are texts to process\n",
    "            chunk_embeddings = get_bert_embeddings(chunk_texts)\n",
    "            level_predictions = loaded_champion_classifier.predict(chunk_embeddings)\n",
    "            chunk_df['predicted_polarization_level'] = level_predictions\n",
    "            \n",
    "            # Save the individual processed chunk\n",
    "            chunk_df.to_csv(chunk_output_path, index=False)\n",
    "            print(f\" Chunk {chunk_index + 1} processed and saved to '{chunk_output_path}'\")\n",
    "        else:\n",
    "            print(f\"No data found for Chunk {chunk_index + 1}. It might be an empty last chunk.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing Chunk {chunk_index + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486a644-79aa-44bb-b164-ae704c33a642",
   "metadata": {},
   "source": [
    "**Combine all the parts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab7d01b-8820-4c36-8c0a-edf16f305a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Combining all processed chunks into the final file\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Combining all processed chunks into the final file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Find all the completed chunk files\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m chunk_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TEMP_CHUNK_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk_files) \u001b[38;5;241m!=\u001b[39m NUM_CHUNKS:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m WARNING: Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_CHUNKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunk files, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n Combining all processed chunks into the final file\")\n",
    "\n",
    "# Find all the completed chunk files\n",
    "chunk_files = sorted(glob.glob(os.path.join(TEMP_CHUNK_DIR, 'chunk_*.csv')))\n",
    "\n",
    "if len(chunk_files) != NUM_CHUNKS:\n",
    "    print(f\" WARNING: Expected {NUM_CHUNKS} chunk files, but found {len(chunk_files)}.\")\n",
    "    print(\"Please ensure all chunks were processed before combining.\")\n",
    "else:\n",
    "    print(f\"Found {len(chunk_files)} chunk files. Merging now...\")\n",
    "    \n",
    "    # Use an iterator and a loop to append chunks\n",
    "    with open(FINAL_OUTPUT_FILENAME, 'w', encoding='utf-8') as outfile:\n",
    "        # Write the header using the first file's columns\n",
    "        pd.read_csv(chunk_files[0], nrows=0).to_csv(outfile, index=False)\n",
    "        \n",
    "        # Write data from all chunk files\n",
    "        for filename in tqdm(chunk_files, desc=\"Merging Chunks\"):\n",
    "            chunk_data = pd.read_csv(filename)\n",
    "            chunk_data.to_csv(outfile, mode='a', header=False, index=False)\n",
    "\n",
    "    print(f\"\\n All chunks combined into '{FINAL_OUTPUT_FILENAME}'\")\n",
    "    print(\"You can now safely delete the 'temp_chunks' directory if you wish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f44215f6-1ef2-4d2f-83f3-de3b5213e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and analyzing 'debate_tweets_classified_COMPLETE.csv'\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# --- Run the Evaluation ---\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Replace with the actual path to your file if it's in a different directory\u001b[39;00m\n\u001b[1;32m     44\u001b[0m file_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebate_tweets_classified_COMPLETE.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m evaluate_dataframe(file_to_evaluate)\n",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m, in \u001b[0;36mevaluate_dataframe\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading and analyzing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(filepath, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# File Structure and Shape\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile Structure and Shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32mparsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:914\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/codecs.py:334\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.getstate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    331\u001b[0m     IncrementalDecoder\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;66;03m# additional state info is always 0\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# ignore additional state info\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "def evaluate_dataframe(filepath):\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading and analyzing '{filepath}'\\n\")\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "\n",
    "        # File Structure and Shape\n",
    "        print(\"File Structure and Shape\")\n",
    "        rows, cols = df.shape\n",
    "        print(f\"Total Rows: {rows}\")\n",
    "        print(f\"Total Columns: {cols}\\n\")\n",
    "\n",
    "        # Column Analysis \n",
    "        print(\"Column Analysis (Name, Non-Null Count, Dtype)\")\n",
    "        # .info() provides a concise summary of the DataFrame\n",
    "        # We capture its output to print it directly.\n",
    "        buffer = io.StringIO()\n",
    "        df.info(buf=buffer)\n",
    "        info_str = buffer.getvalue()\n",
    "        print(info_str)\n",
    "\n",
    "        # Data Sample\n",
    "        print(\"\\nData Sample\")\n",
    "        # Display a sample with key columns for a quick look at the content\n",
    "        sample_cols = [\n",
    "            'tweet_id', 'author_id', 'created_at', 'text', 'santext',\n",
    "            'predicted_polarization_level'\n",
    "        ]\n",
    "        # Ensure the columns exist before trying to display them\n",
    "        display_cols = [col for col in sample_cols if col in df.columns]\n",
    "        print(df[display_cols].head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at the path: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- Run the Evaluation ---\n",
    "# Replace with the actual path to your file if it's in a different directory\n",
    "file_to_evaluate = 'debate_tweets_classified_COMPLETE.csv'\n",
    "evaluate_dataframe(file_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4dc04-7c7b-4498-9b51-b67d1cc4b827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa33e14-1346-4ee7-9ac8-f5cb9e5a1943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataframe(filepath):\n",
    "   \n",
    "    try:\n",
    "        print(f\"Loading and analyzing '{filepath}'\\n\")\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "\n",
    "        # File Structure and Shape\n",
    "        print(\"File Structure and Shape\")\n",
    "        rows, cols = df.shape\n",
    "        print(f\"Total Rows: {rows}\")\n",
    "        print(f\"Total Columns: {cols}\\n\")\n",
    "\n",
    "        # Column Analysis\n",
    "        print(\"Column Analysis (Name, Non-Null Count, Dtype)\")\n",
    "        # .info() provides a concise summary of the DataFrame\n",
    "        # We capture its output to print it directly.\n",
    "        buffer = io.StringIO()\n",
    "        df.info(buf=buffer)\n",
    "        info_str = buffer.getvalue()\n",
    "        print(info_str)\n",
    "\n",
    "        # Data Sample\n",
    "        print(\"\\nData Sample\")\n",
    "        # Display a sample with key columns for a quick look at the content\n",
    "        sample_cols = [\n",
    "            'tweet_id', 'author_id', 'created_at', 'text', 'santext',\n",
    "            'predicted_polarization_level'\n",
    "        ]\n",
    "        # Ensure the columns exist before trying to display them\n",
    "        display_cols = [col for col in sample_cols if col in df.columns]\n",
    "        print(df[display_cols].head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at the path: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- Run the Evaluation ---\n",
    "# Replace with the actual path to your file if it's in a different directory\n",
    "file_to_evaluate = 'data/processed/debate_tweets_with_polarization_and_sentiment.csv'\n",
    "evaluate_dataframe(file_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93a3ea-dbd1-4539-b8e3-b9720c355803",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lhra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d4005-b50a-4005-85a6-bec3e5bdb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .gitignore\n",
    "secrets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76c1fb-2d90-4f28-8269-d54a219f0130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1202cf-fcd1-42dd-8bff-15d0454633ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
